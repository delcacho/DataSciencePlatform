c = get_config()
#c.InteractiveShellApp.extensions.append('sparkmonitor.kernelextension')
c.InteractiveShellApp.exec_lines = [
    "from pyspark import SparkConf",
    "from pyspark.sql import SparkSession",
    "from pyspark import SparkContext",
    "from dask.distributed import Client",
    "conf = SparkConf()",
    "conf.set('spark.jars.repositories', 'https://mmlspark.azureedge.net/maven')", 
    "conf.set('spark.driver.port','29413')",
    "conf.set('spark.jars.packages', 'io.delta:delta-core_2.11:0.6.1,com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc3,com.microsoft.azure:azure-storage:2.2.0,org.apache.hadoop:hadoop-azure:2.7.7,com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.7,org.apache.hadoop:hadoop-client:2.7.7')",
    "conf.set('spark.sql.repl.eagerEval.enabled', True)",
    "conf.set('spark.databricks.delta.retentionDurationCheck.enabled','false')",
    "conf.set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')",
    "conf.set('spark.sql.sources.parallelPartitionDiscovery.parallelism', '8')",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()",
    "from delta.tables import *",
    "if spark.sparkContext.version < '3.':",
    "  spark.sparkContext._jvm.io.delta.sql.DeltaSparkSessionExtension().apply(spark._jsparkSession.extensions())",
    "spark = SparkSession(spark.sparkContext, spark._jsparkSession.cloneSession())",
    "sc = spark.sparkContext",
    "client = Client('scheduler:8786')",
    "import os",
    "os.environ['MODIN_ENGINE'] = 'dask'",
    "import modin.pandas as pd",
]
