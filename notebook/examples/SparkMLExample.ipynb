{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/databricks/mlflow-example-sklearn-elasticnet-wine/master/wine-quality.csv\"\n",
    "urllib.request.urlretrieve(url, \"wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put wine.csv wine.csv\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark.read.csv(\"wine.csv\", header=\"true\", inferSchema=\"true\")\n",
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# select the columns to be used as the features (all except `quality`)\n",
    "featureColumns = [c for c in inputDF.columns if c != 'quality']\n",
    "\n",
    "# create and configure the assembler\n",
    "assembler = VectorAssembler(inputCols=featureColumns, \n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "trainDF, testDF = inputDF.randomSplit([0.75, 0.25], seed = 30)\n",
    "\n",
    "# fit a `LinearRegression` model using features in colum `features` and label in column `quality`\n",
    "lr = LinearRegression(maxIter=30, regParam=0.3, elasticNetParam=0.3, featuresCol=\"features\", labelCol=\"quality\")\n",
    "pipeline = Pipeline(stages=[assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "#mlflow.spark.autolog()\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2, 0.3, 0.4, 0.5]) \\\n",
    "    .addGrid(lr.regParam, [0.3, 0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "mlflow.set_experiment(\"Experimento Spark 2\")\n",
    "evaluator = RegressionEvaluator(labelCol=\"quality\")\n",
    "validator = CrossValidator(estimator=pipeline,\n",
    "                estimatorParamMaps=paramGrid,\n",
    "                evaluator=evaluator,\n",
    "                numFolds=5)\n",
    "cvModel = validator.fit(trainDF)\n",
    "pipeModel = cvModel.bestModel\n",
    "\n",
    "#print(evaluator.getMetricName())\n",
    "besti = np.argmin(cvModel.avgMetrics)\n",
    "for i,params in enumerate(cvModel.getEstimatorParamMaps()):\n",
    "    with mlflow.start_run() as run:\n",
    "        for k,v in params.items():\n",
    "            mlflow.log_param(k.name, v)\n",
    "        rmse = cvModel.avgMetrics[i]\n",
    "        mlflow.log_metric(\"rmse\",rmse)\n",
    "        if i == besti:\n",
    "           mlflow.spark.log_model(pipeModel,\"sparkmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in zip(featureColumns, pipeModel.stages[1].coefficients):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Spark 2.4.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
