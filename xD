+ unset KAFKA_PORT
+ export KAFKA_BROKER_ID=0
+ export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://100.96.2.11:9092
+ exec /etc/confluent/docker/run


# Set environment values if they exist as arguments
if [ $# -ne 0 ]; then
  echo "===> Overriding env params with args ..."
  for var in "$@"
  do
    export "$var"
  done
fi
+ '[' 0 -ne 0 ']'

echo "===> ENV Variables ..."
+ echo '===> ENV Variables ...'
env | sort
===> ENV Variables ...
ALLOW_UNSIGNED=false
AMBASSADOR_ADMINS_PORT=tcp://100.70.83.126:8877
AMBASSADOR_ADMINS_PORT_8877_TCP=tcp://100.70.83.126:8877
AMBASSADOR_ADMINS_PORT_8877_TCP_ADDR=100.70.83.126
AMBASSADOR_ADMINS_PORT_8877_TCP_PORT=8877
AMBASSADOR_ADMINS_PORT_8877_TCP_PROTO=tcp
AMBASSADOR_ADMINS_SERVICE_HOST=100.70.83.126
AMBASSADOR_ADMINS_SERVICE_PORT=8877
AMBASSADOR_ADMINS_SERVICE_PORT_ADMIN=8877
AMBASSADOR_PORT=tcp://100.64.103.5:80
AMBASSADOR_PORT_443_TCP=tcp://100.64.103.5:443
AMBASSADOR_PORT_443_TCP_ADDR=100.64.103.5
AMBASSADOR_PORT_443_TCP_PORT=443
AMBASSADOR_PORT_443_TCP_PROTO=tcp
AMBASSADOR_PORT_80_TCP=tcp://100.64.103.5:80
AMBASSADOR_PORT_80_TCP_ADDR=100.64.103.5
AMBASSADOR_PORT_80_TCP_PORT=80
AMBASSADOR_PORT_80_TCP_PROTO=tcp
AMBASSADOR_SERVICE_HOST=100.64.103.5
AMBASSADOR_SERVICE_PORT=80
AMBASSADOR_SERVICE_PORT_HTTP=80
AMBASSADOR_SERVICE_PORT_HTTPS=443
COMPONENT=kafka
CONFLUENT_DEB_VERSION=1
CONFLUENT_MAJOR_VERSION=5
CONFLUENT_MINOR_VERSION=0
CONFLUENT_MVN_LABEL=
CONFLUENT_PATCH_VERSION=1
CONFLUENT_PLATFORM_LABEL=
CONFLUENT_VERSION=5.0.1
CUB_CLASSPATH=/etc/confluent/docker/docker-utils.jar
HDFS_HTTPFS_PORT=tcp://100.68.130.198:14000
HDFS_HTTPFS_PORT_14000_TCP=tcp://100.68.130.198:14000
HDFS_HTTPFS_PORT_14000_TCP_ADDR=100.68.130.198
HDFS_HTTPFS_PORT_14000_TCP_PORT=14000
HDFS_HTTPFS_PORT_14000_TCP_PROTO=tcp
HDFS_HTTPFS_SERVICE_HOST=100.68.130.198
HDFS_HTTPFS_SERVICE_PORT=14000
HDFS_HTTPFS_SERVICE_PORT_HTTPFS=14000
HDFS_NAMENODE_EXPORTER_PORT=tcp://100.65.173.78:5556
HDFS_NAMENODE_EXPORTER_PORT_5556_TCP=tcp://100.65.173.78:5556
HDFS_NAMENODE_EXPORTER_PORT_5556_TCP_ADDR=100.65.173.78
HDFS_NAMENODE_EXPORTER_PORT_5556_TCP_PORT=5556
HDFS_NAMENODE_EXPORTER_PORT_5556_TCP_PROTO=tcp
HDFS_NAMENODE_EXPORTER_SERVICE_HOST=100.65.173.78
HDFS_NAMENODE_EXPORTER_SERVICE_PORT=5556
HDFS_NAMENODE_EXPORTER_SERVICE_PORT_METRICS=5556
HDFS_PORT=tcp://100.71.115.141:8020
HDFS_PORT_50070_TCP=tcp://100.71.115.141:50070
HDFS_PORT_50070_TCP_ADDR=100.71.115.141
HDFS_PORT_50070_TCP_PORT=50070
HDFS_PORT_50070_TCP_PROTO=tcp
HDFS_PORT_8020_TCP=tcp://100.71.115.141:8020
HDFS_PORT_8020_TCP_ADDR=100.71.115.141
HDFS_PORT_8020_TCP_PORT=8020
HDFS_PORT_8020_TCP_PROTO=tcp
HDFS_SERVICE_HOST=100.71.115.141
HDFS_SERVICE_PORT=8020
HDFS_SERVICE_PORT_DFS=8020
HDFS_SERVICE_PORT_WEBHDFS=50070
HOME=/root
HOSTNAME=kafka-0
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT=tcp://100.64.21.69:80
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT_443_TCP=tcp://100.64.21.69:443
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_ADDR=100.64.21.69
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_PORT=443
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_PROTO=tcp
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT_80_TCP=tcp://100.64.21.69:80
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_ADDR=100.64.21.69
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_PORT=80
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_PROTO=tcp
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_SERVICE_HOST=100.64.21.69
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_SERVICE_PORT=80
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_SERVICE_PORT_HTTP=80
INGRESS_HTTPS_NGINX_INGRESS_CONTROLLER_SERVICE_PORT_HTTPS=443
INGRESS_HTTPS_NGINX_INGRESS_DEFAULT_BACKEND_PORT=tcp://100.69.108.104:80
INGRESS_HTTPS_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP=tcp://100.69.108.104:80
INGRESS_HTTPS_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_ADDR=100.69.108.104
INGRESS_HTTPS_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_PORT=80
INGRESS_HTTPS_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_PROTO=tcp
INGRESS_HTTPS_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_HOST=100.69.108.104
INGRESS_HTTPS_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_PORT=80
INGRESS_HTTPS_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_PORT_HTTP=80
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT=tcp://100.67.139.114:80
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT_443_TCP=tcp://100.67.139.114:443
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_ADDR=100.67.139.114
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_PORT=443
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_PROTO=tcp
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT_80_TCP=tcp://100.67.139.114:80
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_ADDR=100.67.139.114
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_PORT=80
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_PROTO=tcp
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_SERVICE_HOST=100.67.139.114
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_SERVICE_PORT=80
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_SERVICE_PORT_HTTP=80
INGRESS_HTTP_NGINX_INGRESS_CONTROLLER_SERVICE_PORT_HTTPS=443
INGRESS_HTTP_NGINX_INGRESS_DEFAULT_BACKEND_PORT=tcp://100.68.104.91:80
INGRESS_HTTP_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP=tcp://100.68.104.91:80
INGRESS_HTTP_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_ADDR=100.68.104.91
INGRESS_HTTP_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_PORT=80
INGRESS_HTTP_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_PROTO=tcp
INGRESS_HTTP_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_HOST=100.68.104.91
INGRESS_HTTP_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_PORT=80
INGRESS_HTTP_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_PORT_HTTP=80
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT=tcp://100.71.113.22:80
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT_443_TCP=tcp://100.71.113.22:443
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_ADDR=100.71.113.22
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_PORT=443
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT_443_TCP_PROTO=tcp
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT_80_TCP=tcp://100.71.113.22:80
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_ADDR=100.71.113.22
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_PORT=80
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_PORT_80_TCP_PROTO=tcp
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_SERVICE_HOST=100.71.113.22
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_SERVICE_PORT=80
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_SERVICE_PORT_HTTP=80
INGRESS_SELDON_NGINX_INGRESS_CONTROLLER_SERVICE_PORT_HTTPS=443
INGRESS_SELDON_NGINX_INGRESS_DEFAULT_BACKEND_PORT=tcp://100.69.158.97:80
INGRESS_SELDON_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP=tcp://100.69.158.97:80
INGRESS_SELDON_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_ADDR=100.69.158.97
INGRESS_SELDON_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_PORT=80
INGRESS_SELDON_NGINX_INGRESS_DEFAULT_BACKEND_PORT_80_TCP_PROTO=tcp
INGRESS_SELDON_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_HOST=100.69.158.97
INGRESS_SELDON_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_PORT=80
INGRESS_SELDON_NGINX_INGRESS_DEFAULT_BACKEND_SERVICE_PORT_HTTP=80
JENKINS_AGENT_PORT=tcp://100.71.59.38:50000
JENKINS_AGENT_PORT_50000_TCP=tcp://100.71.59.38:50000
JENKINS_AGENT_PORT_50000_TCP_ADDR=100.71.59.38
JENKINS_AGENT_PORT_50000_TCP_PORT=50000
JENKINS_AGENT_PORT_50000_TCP_PROTO=tcp
JENKINS_AGENT_SERVICE_HOST=100.71.59.38
JENKINS_AGENT_SERVICE_PORT=50000
JENKINS_AGENT_SERVICE_PORT_SLAVELISTENER=50000
JENKINS_PORT=tcp://100.70.169.153:8080
JENKINS_PORT_8080_TCP=tcp://100.70.169.153:8080
JENKINS_PORT_8080_TCP_ADDR=100.70.169.153
JENKINS_PORT_8080_TCP_PORT=8080
JENKINS_PORT_8080_TCP_PROTO=tcp
JENKINS_SERVICE_HOST=100.70.169.153
JENKINS_SERVICE_PORT=8080
JENKINS_SERVICE_PORT_HTTP=8080
KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://100.96.2.11:9092
KAFKA_BROKER_ID=0
KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE=false
KAFKA_HEAP_OPTS=-Xmx1G -Xms1G
KAFKA_JMX_PORT=5555
KAFKA_LOG_DIRS=/opt/kafka/data/logs
KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=3
KAFKA_PORT_9092_TCP=tcp://100.67.186.10:9092
KAFKA_PORT_9092_TCP_ADDR=100.67.186.10
KAFKA_PORT_9092_TCP_PORT=9092
KAFKA_PORT_9092_TCP_PROTO=tcp
KAFKA_SERVICE_HOST=100.67.186.10
KAFKA_SERVICE_PORT=9092
KAFKA_SERVICE_PORT_BROKER=9092
KAFKA_VERSION=2.0.1
KAFKA_ZOOKEEPER_CONNECT=kafka-zookeeper:2181
KAFKA_ZOOKEEPER_PORT=tcp://100.65.239.46:2181
KAFKA_ZOOKEEPER_PORT_2181_TCP=tcp://100.65.239.46:2181
KAFKA_ZOOKEEPER_PORT_2181_TCP_ADDR=100.65.239.46
KAFKA_ZOOKEEPER_PORT_2181_TCP_PORT=2181
KAFKA_ZOOKEEPER_PORT_2181_TCP_PROTO=tcp
KAFKA_ZOOKEEPER_SERVICE_HOST=100.65.239.46
KAFKA_ZOOKEEPER_SERVICE_PORT=2181
KAFKA_ZOOKEEPER_SERVICE_PORT_CLIENT=2181
KUBERNETES_PORT=tcp://100.64.0.1:443
KUBERNETES_PORT_443_TCP=tcp://100.64.0.1:443
KUBERNETES_PORT_443_TCP_ADDR=100.64.0.1
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_SERVICE_HOST=100.64.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
LANG=C.UTF-8
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
POD_IP=100.96.2.11
POD_NAME=kafka-0
POD_NAMESPACE=default
PWD=/
PYTHON_PIP_VERSION=8.1.2
PYTHON_VERSION=2.7.9-1
SCALA_VERSION=2.11
SCHEDULER_PORT=tcp://100.68.123.205:8786
SCHEDULER_PORT_8786_TCP=tcp://100.68.123.205:8786
SCHEDULER_PORT_8786_TCP_ADDR=100.68.123.205
SCHEDULER_PORT_8786_TCP_PORT=8786
SCHEDULER_PORT_8786_TCP_PROTO=tcp
SCHEDULER_PORT_8787_TCP=tcp://100.68.123.205:8787
SCHEDULER_PORT_8787_TCP_ADDR=100.68.123.205
SCHEDULER_PORT_8787_TCP_PORT=8787
SCHEDULER_PORT_8787_TCP_PROTO=tcp
SCHEDULER_SERVICE_HOST=100.68.123.205
SCHEDULER_SERVICE_PORT=8786
SCHEDULER_SERVICE_PORT_8786=8786
SCHEDULER_SERVICE_PORT_8787=8787
SHLVL=1
ZULU_OPENJDK_VERSION=8=8.30.0.1
_=/usr/bin/env
+ env
+ sort
===> User

echo "===> User"
+ echo '===> User'
id
+ id

echo "===> Configuring ..."
+ echo '===> Configuring ...'
/etc/confluent/docker/configure
+ /etc/confluent/docker/configure

dub ensure KAFKA_ZOOKEEPER_CONNECT
+ dub ensure KAFKA_ZOOKEEPER_CONNECT
uid=0(root) gid=0(root) groups=0(root)
===> Configuring ...
dub ensure KAFKA_ADVERTISED_LISTENERS
+ dub ensure KAFKA_ADVERTISED_LISTENERS

# By default, LISTENERS is derived from ADVERTISED_LISTENERS by replacing
# hosts with 0.0.0.0. This is good default as it ensures that the broker
# process listens on all ports.
if [[ -z "${KAFKA_LISTENERS-}" ]]
then
  export KAFKA_LISTENERS
  KAFKA_LISTENERS=$(cub listeners "$KAFKA_ADVERTISED_LISTENERS")
fi
+ [[ -z '' ]]
+ export KAFKA_LISTENERS
cub listeners "$KAFKA_ADVERTISED_LISTENERS"
++ cub listeners PLAINTEXT://100.96.2.11:9092
+ KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092

dub path /etc/kafka/ writable
+ dub path /etc/kafka/ writable

if [[ -z "${KAFKA_LOG_DIRS-}" ]]
then
  export KAFKA_LOG_DIRS
  KAFKA_LOG_DIRS="/var/lib/kafka/data"
fi
+ [[ -z /opt/kafka/data/logs ]]

# advertised.host, advertised.port, host and port are deprecated. Exit if these properties are set.
if [[ -n "${KAFKA_ADVERTISED_PORT-}" ]]
then
  echo "advertised.port is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."
  exit 1
fi
+ [[ -n '' ]]

if [[ -n "${KAFKA_ADVERTISED_HOST-}" ]]
then
  echo "advertised.host is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."
  exit 1
fi
+ [[ -n '' ]]

if [[ -n "${KAFKA_HOST-}" ]]
then
  echo "host is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."
  exit 1
fi
+ [[ -n '' ]]

if [[ -n "${KAFKA_PORT-}" ]]
then
  echo "port is deprecated. Please use KAFKA_ADVERTISED_LISTENERS instead."
  exit 1
fi
+ [[ -n '' ]]

# Set if ADVERTISED_LISTENERS has SSL:// or SASL_SSL:// endpoints.
if [[ $KAFKA_ADVERTISED_LISTENERS == *"SSL://"* ]]
then
  echo "SSL is enabled."

  dub ensure KAFKA_SSL_KEYSTORE_FILENAME
  export KAFKA_SSL_KEYSTORE_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEYSTORE_FILENAME"
  dub path "$KAFKA_SSL_KEYSTORE_LOCATION" exists

  dub ensure KAFKA_SSL_KEY_CREDENTIALS
  KAFKA_SSL_KEY_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEY_CREDENTIALS"
  dub path "$KAFKA_SSL_KEY_CREDENTIALS_LOCATION" exists
  export KAFKA_SSL_KEY_PASSWORD
  KAFKA_SSL_KEY_PASSWORD=$(cat "$KAFKA_SSL_KEY_CREDENTIALS_LOCATION")

  dub ensure KAFKA_SSL_KEYSTORE_CREDENTIALS
  KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_KEYSTORE_CREDENTIALS"
  dub path "$KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION" exists
  export KAFKA_SSL_KEYSTORE_PASSWORD
  KAFKA_SSL_KEYSTORE_PASSWORD=$(cat "$KAFKA_SSL_KEYSTORE_CREDENTIALS_LOCATION")

  if [[ -n "${KAFKA_SSL_CLIENT_AUTH-}" ]] && ( [[ $KAFKA_SSL_CLIENT_AUTH == *"required"* ]] || [[ $KAFKA_SSL_CLIENT_AUTH == *"requested"* ]] )
  then
      dub ensure KAFKA_SSL_TRUSTSTORE_FILENAME
      export KAFKA_SSL_TRUSTSTORE_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_TRUSTSTORE_FILENAME"
      dub path "$KAFKA_SSL_TRUSTSTORE_LOCATION" exists

      dub ensure KAFKA_SSL_TRUSTSTORE_CREDENTIALS
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION="/etc/kafka/secrets/$KAFKA_SSL_TRUSTSTORE_CREDENTIALS"
      dub path "$KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION" exists
      export KAFKA_SSL_TRUSTSTORE_PASSWORD
      KAFKA_SSL_TRUSTSTORE_PASSWORD=$(cat "$KAFKA_SSL_TRUSTSTORE_CREDENTIALS_LOCATION")
  fi
  
fi
+ [[ PLAINTEXT://100.96.2.11:9092 == *\S\S\L\:\/\/* ]]

# Set if KAFKA_ADVERTISED_LISTENERS has SASL_PLAINTEXT:// or SASL_SSL:// endpoints.
if [[ $KAFKA_ADVERTISED_LISTENERS =~ .*SASL_.*://.* ]]
then
  echo "SASL" is enabled.

  dub ensure KAFKA_OPTS

  if [[ ! $KAFKA_OPTS == *"java.security.auth.login.config"*  ]]
  then
    echo "KAFKA_OPTS should contain 'java.security.auth.login.config' property."
  fi
fi
+ [[ PLAINTEXT://100.96.2.11:9092 =~ .*SASL_.*://.* ]]

if [[ -n "${KAFKA_JMX_OPTS-}" ]]
then
  if [[ ! $KAFKA_JMX_OPTS == *"com.sun.management.jmxremote.rmi.port"*  ]]
  then
    echo "KAFKA_OPTS should contain 'com.sun.management.jmxremote.rmi.port' property. It is required for accessing the JMX metrics externally."
  fi
fi
+ [[ -n '' ]]

dub template "/etc/confluent/docker/${COMPONENT}.properties.template" "/etc/${COMPONENT}/${COMPONENT}.properties"
+ dub template /etc/confluent/docker/kafka.properties.template /etc/kafka/kafka.properties
dub template "/etc/confluent/docker/log4j.properties.template" "/etc/${COMPONENT}/log4j.properties"
+ dub template /etc/confluent/docker/log4j.properties.template /etc/kafka/log4j.properties
dub template "/etc/confluent/docker/tools-log4j.properties.template" "/etc/${COMPONENT}/tools-log4j.properties"
+ dub template /etc/confluent/docker/tools-log4j.properties.template /etc/kafka/tools-log4j.properties

echo "===> Running preflight checks ... "
+ echo '===> Running preflight checks ... '
===> Running preflight checks ... 
/etc/confluent/docker/ensure
+ /etc/confluent/docker/ensure

export KAFKA_DATA_DIRS=${KAFKA_DATA_DIRS:-"/var/lib/kafka/data"}
+ export KAFKA_DATA_DIRS=/var/lib/kafka/data
+ KAFKA_DATA_DIRS=/var/lib/kafka/data
===> Check if /var/lib/kafka/data is writable ...
echo "===> Check if $KAFKA_DATA_DIRS is writable ..."
+ echo '===> Check if /var/lib/kafka/data is writable ...'
dub path "$KAFKA_DATA_DIRS" writable
+ dub path /var/lib/kafka/data writable
===> Check if Zookeeper is healthy ...

echo "===> Check if Zookeeper is healthy ..."
+ echo '===> Check if Zookeeper is healthy ...'
cub zk-ready "$KAFKA_ZOOKEEPER_CONNECT" "${KAFKA_CUB_ZK_TIMEOUT:-40}"
+ cub zk-ready kafka-zookeeper:2181 40
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=kafka-0.kafka-headless.default.svc.cluster.local
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=1.8.0_172
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/etc/confluent/docker/docker-utils.jar
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=4.9.0-11-amd64
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=root
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/root
[main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/
[main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=kafka-zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@b1bc7ed
[main-SendThread(kafka-zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server kafka-zookeeper/100.65.239.46:2181. Will not attempt to authenticate using SASL (unknown error)
[main-SendThread(kafka-zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established to kafka-zookeeper/100.65.239.46:2181, initiating session
[main-SendThread(kafka-zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server kafka-zookeeper/100.65.239.46:2181, sessionid = 0x200001a0cfd0000, negotiated timeout = 40000
[main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x200001a0cfd0000 closed

echo "===> Launching ... "
+ echo '===> Launching ... '
exec /etc/confluent/docker/launch
+ exec /etc/confluent/docker/launch
===> Launching ... 
===> Launching kafka ... 
[2020-05-13 17:31:48,267] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2020-05-13 17:31:48,515] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://100.96.2.11:9092
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.0-IV1
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /opt/kafka/data/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = kafka-zookeeper:2181
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-05-13 17:31:48,580] WARN The package io.confluent.support.metrics.collectors.FullCollector for collecting the full set of support metrics could not be loaded, so we are reverting to anonymous, basic metric collection. If you are a Confluent customer, please refer to the Confluent Platform documentation, section Proactive Support, on how to activate full metrics collection. (io.confluent.support.metrics.KafkaSupportConfig)
[2020-05-13 17:31:48,580] WARN The support metrics collection feature ("Metrics") of Proactive Support is disabled. (io.confluent.support.metrics.SupportedServerStartable)
[2020-05-13 17:31:48,585] INFO starting (kafka.server.KafkaServer)
[2020-05-13 17:31:48,586] INFO Connecting to zookeeper on kafka-zookeeper:2181 (kafka.server.KafkaServer)
[2020-05-13 17:31:48,601] INFO [ZooKeeperClient] Initializing a new session to kafka-zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[2020-05-13 17:31:48,606] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:host.name=kafka-0.kafka-headless.default.svc.cluster.local (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:java.version=1.8.0_172 (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/jline-0.9.94.jar:/usr/bin/../share/java/kafka/commons-codec-1.9.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.27.jar:/usr/bin/../share/java/kafka/httpclient-4.5.2.jar:/usr/bin/../share/java/kafka/lz4-java-1.4.1.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.7.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/xz-1.5.jar:/usr/bin/../share/java/kafka/httpcore-4.4.4.jar:/usr/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/usr/bin/../share/java/kafka/kafka-tools-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.9.7.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.1-cp1-test.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.1.jar:/usr/bin/../share/java/kafka/commons-lang3-3.5.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.27.jar:/usr/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/usr/bin/../share/java/kafka/connect-transforms-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.9.7.jar:/usr/bin/../share/java/kafka/zookeeper-3.4.13.jar:/usr/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/support-metrics-common-5.0.1.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.27.jar:/usr/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/httpmime-4.5.2.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.1-cp1-sources.jar:/usr/bin/../share/java/kafka/scala-library-2.11.12.jar:/usr/bin/../share/java/kafka/commons-compress-1.8.1.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/javassist-3.22.0-CR2.jar:/usr/bin/../share/java/kafka/support-metrics-client-5.0.1.jar:/usr/bin/../share/java/kafka/avro-1.8.1.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jackson-databind-2.9.7.jar:/usr/bin/../share/java/kafka/javax.inject-2.5.0-b42.jar:/usr/bin/../share/java/kafka/hk2-locator-2.5.0-b42.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/paranamer-2.7.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.1-cp1-scaladoc.jar:/usr/bin/../share/java/kafka/connect-api-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.1-cp1-test-sources.jar:/usr/bin/../share/java/kafka/commons-lang3-3.1.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.27.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/kafka-clients-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/reflections-0.9.11.jar:/usr/bin/../share/java/kafka/commons-validator-1.4.1.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.7.jar:/usr/bin/../share/java/kafka/commons-collections-3.2.1.jar:/usr/bin/../share/java/kafka/jackson-core-2.9.7.jar:/usr/bin/../share/java/kafka/guava-20.0.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/netty-3.10.6.Final.jar:/usr/bin/../share/java/kafka/log4j-1.2.17.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/connect-runtime-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/connect-json-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/jersey-common-2.27.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.11.v20180605.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/hk2-api-2.5.0-b42.jar:/usr/bin/../share/java/kafka/hk2-utils-2.5.0-b42.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.1-cp1-javadoc.jar:/usr/bin/../share/java/kafka/maven-artifact-3.5.3.jar:/usr/bin/../share/java/kafka/scala-logging_2.11-3.9.0.jar:/usr/bin/../share/java/kafka/common-utils-5.0.1.jar:/usr/bin/../share/java/kafka/zkclient-0.10.jar:/usr/bin/../share/java/kafka/jersey-client-2.27.jar:/usr/bin/../share/java/kafka/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/kafka_2.11-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.11-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/javax.inject-1.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b42.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/connect-file-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/kafka-streams-2.0.1-cp1.jar:/usr/bin/../share/java/kafka/jersey-server-2.27.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.7.3.jar:/usr/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/usr/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,606] INFO Client environment:os.version=4.9.0-11-amd64 (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,607] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,607] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,607] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,608] INFO Initiating client connection, connectString=kafka-zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@145f66e3 (org.apache.zookeeper.ZooKeeper)
[2020-05-13 17:31:48,621] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2020-05-13 17:31:48,622] INFO Opening socket connection to server kafka-zookeeper/100.65.239.46:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2020-05-13 17:31:48,627] INFO Socket connection established to kafka-zookeeper/100.65.239.46:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2020-05-13 17:31:48,639] INFO Session establishment complete on server kafka-zookeeper/100.65.239.46:2181, sessionid = 0x1000019e7480000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2020-05-13 17:31:48,642] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2020-05-13 17:31:48,981] INFO Cluster ID = D5HItewGScmsCjmbuR9w9Q (kafka.server.KafkaServer)
[2020-05-13 17:31:48,987] WARN No meta.properties file under dir /opt/kafka/data/logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2020-05-13 17:31:49,033] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://100.96.2.11:9092
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.0-IV1
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /opt/kafka/data/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = kafka-zookeeper:2181
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-05-13 17:31:49,040] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = PLAINTEXT://100.96.2.11:9092
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.0-IV1
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://0.0.0.0:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /opt/kafka/data/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = kafka-zookeeper:2181
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-05-13 17:31:49,063] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-05-13 17:31:49,064] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-05-13 17:31:49,066] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-05-13 17:31:49,092] INFO Log directory /opt/kafka/data/logs not found, creating it. (kafka.log.LogManager)
[2020-05-13 17:31:49,100] INFO Loading logs. (kafka.log.LogManager)
[2020-05-13 17:31:49,107] INFO Logs loading complete in 7 ms. (kafka.log.LogManager)
[2020-05-13 17:31:49,121] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2020-05-13 17:31:49,123] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2020-05-13 17:31:49,126] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2020-05-13 17:31:49,295] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2020-05-13 17:31:49,555] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2020-05-13 17:31:49,582] INFO [SocketServer brokerId=0] Started 1 acceptor threads (kafka.network.SocketServer)
[2020-05-13 17:31:49,599] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-05-13 17:31:49,600] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-05-13 17:31:49,603] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-05-13 17:31:49,620] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2020-05-13 17:31:49,646] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2020-05-13 17:31:49,650] INFO Result of znode creation at /brokers/ids/0 is: OK (kafka.zk.KafkaZkClient)
[2020-05-13 17:31:49,651] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(100.96.2.11,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2020-05-13 17:31:49,653] WARN No meta.properties file under dir /opt/kafka/data/logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2020-05-13 17:31:49,710] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
[2020-05-13 17:31:49,721] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-05-13 17:31:49,721] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-05-13 17:31:49,727] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[2020-05-13 17:31:49,733] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-05-13 17:31:49,742] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[2020-05-13 17:31:49,743] INFO [Controller id=0] 0 successfully elected as the controller (kafka.controller.KafkaController)
[2020-05-13 17:31:49,743] INFO [Controller id=0] Reading controller epoch from ZooKeeper (kafka.controller.KafkaController)
[2020-05-13 17:31:49,745] INFO [Controller id=0] Incrementing controller epoch in ZooKeeper (kafka.controller.KafkaController)
[2020-05-13 17:31:49,757] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2020-05-13 17:31:49,757] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2020-05-13 17:31:49,761] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:31:49,770] INFO [Controller id=0] Epoch incremented to 1 (kafka.controller.KafkaController)
[2020-05-13 17:31:49,771] INFO [Controller id=0] Registering handlers (kafka.controller.KafkaController)
[2020-05-13 17:31:49,784] INFO [Controller id=0] Deleting log dir event notifications (kafka.controller.KafkaController)
[2020-05-13 17:31:49,788] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2020-05-13 17:31:49,792] INFO [Controller id=0] Deleting isr change notifications (kafka.controller.KafkaController)
[2020-05-13 17:31:49,796] INFO [Controller id=0] Initializing controller context (kafka.controller.KafkaController)
[2020-05-13 17:31:49,821] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-05-13 17:31:49,834] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-05-13 17:31:49,846] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2020-05-13 17:31:49,873] DEBUG [Controller id=0] Register BrokerModifications handler for Set(0) (kafka.controller.KafkaController)
[2020-05-13 17:31:49,884] DEBUG [Channel manager on controller 0]: Controller 0 trying to connect to broker 0 (kafka.controller.ControllerChannelManager)
[2020-05-13 17:31:49,903] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread)
[2020-05-13 17:31:49,906] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2020-05-13 17:31:49,908] INFO [Controller id=0] Partitions being reassigned: Map() (kafka.controller.KafkaController)
[2020-05-13 17:31:49,909] INFO [Controller id=0] Currently active brokers in the cluster: Set(0) (kafka.controller.KafkaController)
[2020-05-13 17:31:49,910] INFO [Controller id=0] Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController)
[2020-05-13 17:31:49,913] INFO [Controller id=0] Current list of topics in the cluster: Set() (kafka.controller.KafkaController)
[2020-05-13 17:31:49,913] INFO [Controller id=0] Fetching topic deletions in progress (kafka.controller.KafkaController)
[2020-05-13 17:31:49,917] INFO [Controller id=0] List of topics to be deleted:  (kafka.controller.KafkaController)
[2020-05-13 17:31:49,917] INFO [Controller id=0] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
[2020-05-13 17:31:49,918] INFO [Controller id=0] Initializing topic deletion manager (kafka.controller.KafkaController)
[2020-05-13 17:31:49,919] INFO [Controller id=0] Sending update metadata request (kafka.controller.KafkaController)
[2020-05-13 17:31:49,924] INFO [SocketServer brokerId=0] Started processors for 1 acceptors (kafka.network.SocketServer)
[2020-05-13 17:31:49,926] INFO Kafka version : 2.0.1-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2020-05-13 17:31:49,934] INFO Kafka commitId : 815feb8a888d39d9 (org.apache.kafka.common.utils.AppInfoParser)
[2020-05-13 17:31:49,938] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2020-05-13 17:31:49,942] INFO [RequestSendThread controllerId=0] Controller 0 connected to 100.96.2.11:9092 (id: 0 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
[2020-05-13 17:31:49,939] INFO [ReplicaStateMachine controllerId=0] Initializing replica state (kafka.controller.ReplicaStateMachine)
[2020-05-13 17:31:49,943] INFO [ReplicaStateMachine controllerId=0] Triggering online replica state changes (kafka.controller.ReplicaStateMachine)
[2020-05-13 17:31:49,948] INFO [ReplicaStateMachine controllerId=0] Started replica state machine with initial state -> Map() (kafka.controller.ReplicaStateMachine)
[2020-05-13 17:31:49,949] INFO [PartitionStateMachine controllerId=0] Initializing partition state (kafka.controller.PartitionStateMachine)
[2020-05-13 17:31:49,950] INFO [PartitionStateMachine controllerId=0] Triggering online partition state changes (kafka.controller.PartitionStateMachine)
[2020-05-13 17:31:49,960] INFO [PartitionStateMachine controllerId=0] Started partition state machine with initial state -> Map() (kafka.controller.PartitionStateMachine)
[2020-05-13 17:31:49,960] INFO [Controller id=0] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
[2020-05-13 17:31:49,985] INFO [Controller id=0] Removing partitions Set() from the list of reassigned partitions in zookeeper (kafka.controller.KafkaController)
[2020-05-13 17:31:49,994] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 0 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:31:49,994] INFO [Controller id=0] No more partitions need to be reassigned. Deleting zk path /admin/reassign_partitions (kafka.controller.KafkaController)
[2020-05-13 17:31:50,004] INFO [Controller id=0] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
[2020-05-13 17:31:50,005] INFO [Controller id=0] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
[2020-05-13 17:31:50,005] INFO [Controller id=0] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
[2020-05-13 17:31:50,006] INFO [Controller id=0] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
[2020-05-13 17:31:50,006] INFO [Controller id=0] Starting preferred replica leader election for partitions  (kafka.controller.KafkaController)
[2020-05-13 17:31:50,011] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController)
[2020-05-13 17:31:55,013] TRACE [Controller id=0] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2020-05-13 17:31:55,015] DEBUG [Controller id=0] Preferred replicas by broker Map() (kafka.controller.KafkaController)
[2020-05-13 17:32:53,920] INFO [Controller id=0] Newly added brokers: 1, deleted brokers: , all live brokers: 0,1 (kafka.controller.KafkaController)
[2020-05-13 17:32:53,921] DEBUG [Channel manager on controller 0]: Controller 0 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
[2020-05-13 17:32:53,929] INFO [Controller id=0] New broker startup callback for 1 (kafka.controller.KafkaController)
[2020-05-13 17:32:53,932] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 1 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:32:53,934] DEBUG [Controller id=0] Register BrokerModifications handler for ArrayBuffer(1) (kafka.controller.KafkaController)
[2020-05-13 17:32:53,934] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread)
[2020-05-13 17:32:53,935] INFO [RequestSendThread controllerId=0] Controller 0 connected to 100.96.5.7:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
[2020-05-13 17:32:54,126] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 0 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:05,641] INFO [Controller id=0] New topics: [Set(connect-offsets)], deleted topics: [Set()], new partition replica assignment [Map(connect-offsets-0 -> Vector(1))] (kafka.controller.KafkaController)
[2020-05-13 17:33:05,642] INFO [Controller id=0] New partition creation callback for connect-offsets-0 (kafka.controller.KafkaController)
[2020-05-13 17:33:05,647] TRACE [Controller id=0 epoch=1] Changed partition connect-offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2020-05-13 17:33:05,653] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition connect-offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:05,679] TRACE [Controller id=0 epoch=1] Changed partition connect-offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:05,680] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition connect-offsets-0 (state.change.logger)
[2020-05-13 17:33:05,682] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(0, 1) for partition connect-offsets-0 (state.change.logger)
[2020-05-13 17:33:05,684] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition connect-offsets-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:05,686] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2 (state.change.logger)
[2020-05-13 17:33:05,686] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 2 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:33:05,784] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partitions=[{topic=connect-offsets,partition=0,error_code=0}]} for request LEADER_AND_ISR with correlation id 1 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:05,791] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 2 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:08,373] INFO [Controller id=0] New topics: [Set(connect-configs)], deleted topics: [Set()], new partition replica assignment [Map(connect-configs-0 -> Vector(0))] (kafka.controller.KafkaController)
[2020-05-13 17:33:08,374] INFO [Controller id=0] New partition creation callback for connect-configs-0 (kafka.controller.KafkaController)
[2020-05-13 17:33:08,374] TRACE [Controller id=0 epoch=1] Changed partition connect-configs-0 state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2020-05-13 17:33:08,374] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition connect-configs-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:08,388] TRACE [Controller id=0 epoch=1] Changed partition connect-configs-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:08,388] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0, zkVersion=0, replicas=0, isNew=true) to broker 0 for partition connect-configs-0 (state.change.logger)
[2020-05-13 17:33:08,388] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0, 1) for partition connect-configs-0 (state.change.logger)
[2020-05-13 17:33:08,390] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition connect-configs-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:08,391] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 3 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:08,391] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0, zkVersion=0, replicas=0, isNew=true) correlation id 3 from controller 0 epoch 1 for partition connect-configs-0 (state.change.logger)
[2020-05-13 17:33:08,395] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 epoch 1 starting the become-leader transition for partition connect-configs-0 (state.change.logger)
[2020-05-13 17:33:08,396] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions connect-configs-0 (kafka.server.ReplicaFetcherManager)
[2020-05-13 17:33:08,434] INFO [Log partition=connect-configs-0, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:08,439] INFO [Log partition=connect-configs-0, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 25 ms (kafka.log.Log)
[2020-05-13 17:33:08,441] INFO Created log for partition connect-configs-0 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:08,442] INFO [Partition connect-configs-0 broker=0] No checkpointed highwatermark is found for partition connect-configs-0 (kafka.cluster.Partition)
[2020-05-13 17:33:08,444] INFO Replica loaded for partition connect-configs-0 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:08,446] INFO [Partition connect-configs-0 broker=0] connect-configs-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:08,458] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 3 for partition connect-configs-0 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:08,459] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 0 epoch 1 for the become-leader transition for partition connect-configs-0 (state.change.logger)
[2020-05-13 17:33:08,463] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2020-05-13 17:33:08,466] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partitions=[{topic=connect-configs,partition=0,error_code=0}]} for request LEADER_AND_ISR with correlation id 3 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:33:08,467] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition connect-configs-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4 (state.change.logger)
[2020-05-13 17:33:08,468] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 4 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:33:11,041] INFO [Controller id=0] New topics: [Set(connect-status)], deleted topics: [Set()], new partition replica assignment [Map(connect-status-0 -> Vector(1))] (kafka.controller.KafkaController)
[2020-05-13 17:33:11,041] INFO [Controller id=0] New partition creation callback for connect-status-0 (kafka.controller.KafkaController)
[2020-05-13 17:33:11,041] TRACE [Controller id=0 epoch=1] Changed partition connect-status-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2020-05-13 17:33:11,041] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition connect-status-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:11,054] TRACE [Controller id=0 epoch=1] Changed partition connect-status-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:11,054] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition connect-status-0 (state.change.logger)
[2020-05-13 17:33:11,054] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(0, 1) for partition connect-status-0 (state.change.logger)
[2020-05-13 17:33:11,055] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition connect-status-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:11,056] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-status-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5 (state.change.logger)
[2020-05-13 17:33:11,056] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 5 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:33:11,076] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partitions=[{topic=connect-status,partition=0,error_code=0}]} for request LEADER_AND_ISR with correlation id 4 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:11,084] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 5 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:35,917] INFO [Admin Manager on Broker 0]: Error processing create topic request for topic connect-offsets with arguments (numPartitions=25, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=compact}) (kafka.server.AdminManager)
org.apache.kafka.common.errors.TopicExistsException: Topic 'connect-offsets' already exists.
[2020-05-13 17:33:36,311] INFO [Admin Manager on Broker 0]: Error processing create topic request for topic connect-status with arguments (numPartitions=5, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=compact}) (kafka.server.AdminManager)
org.apache.kafka.common.errors.TopicExistsException: Topic 'connect-status' already exists.
[2020-05-13 17:33:36,594] INFO [Admin Manager on Broker 0]: Error processing create topic request for topic connect-configs with arguments (numPartitions=1, replicationFactor=1, replicasAssignments={}, configs={cleanup.policy=compact}) (kafka.server.AdminManager)
org.apache.kafka.common.errors.TopicExistsException: Topic 'connect-configs' already exists.
[2020-05-13 17:33:58,146] INFO [Controller id=0] Newly added brokers: 2, deleted brokers: , all live brokers: 0,1,2 (kafka.controller.KafkaController)
[2020-05-13 17:33:58,146] DEBUG [Channel manager on controller 0]: Controller 0 trying to connect to broker 2 (kafka.controller.ControllerChannelManager)
[2020-05-13 17:33:58,149] INFO [Controller id=0] New broker startup callback for 2 (kafka.controller.KafkaController)
[2020-05-13 17:33:58,150] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition connect-offsets-0 (state.change.logger)
[2020-05-13 17:33:58,150] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition connect-status-0 (state.change.logger)
[2020-05-13 17:33:58,150] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition connect-configs-0 (state.change.logger)
[2020-05-13 17:33:58,152] DEBUG [Controller id=0] Register BrokerModifications handler for ArrayBuffer(2) (kafka.controller.KafkaController)
[2020-05-13 17:33:58,152] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 6 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:58,154] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0], zkVersion=0, replicas=[0], offlineReplicas=[]) for partition connect-configs-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger)
[2020-05-13 17:33:58,154] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger)
[2020-05-13 17:33:58,154] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-status-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6 (state.change.logger)
[2020-05-13 17:33:58,155] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 6 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:33:58,155] INFO [RequestSendThread controllerId=0] Starting (kafka.controller.RequestSendThread)
[2020-05-13 17:33:58,156] INFO [RequestSendThread controllerId=0] Controller 0 connected to 100.96.4.12:9092 (id: 2 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
[2020-05-13 17:33:58,253] INFO [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [Set()], new partition replica assignment [Map(__consumer_offsets-22 -> Vector(1, 0, 2), __consumer_offsets-30 -> Vector(0, 1, 2), __consumer_offsets-8 -> Vector(2, 0, 1), __consumer_offsets-21 -> Vector(0, 2, 1), __consumer_offsets-4 -> Vector(1, 0, 2), __consumer_offsets-27 -> Vector(0, 2, 1), __consumer_offsets-7 -> Vector(1, 2, 0), __consumer_offsets-9 -> Vector(0, 2, 1), __consumer_offsets-46 -> Vector(1, 0, 2), __consumer_offsets-25 -> Vector(1, 2, 0), __consumer_offsets-35 -> Vector(2, 1, 0), __consumer_offsets-41 -> Vector(2, 1, 0), __consumer_offsets-33 -> Vector(0, 2, 1), __consumer_offsets-23 -> Vector(2, 1, 0), __consumer_offsets-49 -> Vector(1, 2, 0), __consumer_offsets-47 -> Vector(2, 1, 0), __consumer_offsets-16 -> Vector(1, 0, 2), __consumer_offsets-28 -> Vector(1, 0, 2), __consumer_offsets-31 -> Vector(1, 2, 0), __consumer_offsets-36 -> Vector(0, 1, 2), __consumer_offsets-42 -> Vector(0, 1, 2), __consumer_offsets-3 -> Vector(0, 2, 1), __consumer_offsets-18 -> Vector(0, 1, 2), __consumer_offsets-37 -> Vector(1, 2, 0), __consumer_offsets-15 -> Vector(0, 2, 1), __consumer_offsets-24 -> Vector(0, 1, 2), __consumer_offsets-38 -> Vector(2, 0, 1), __consumer_offsets-17 -> Vector(2, 1, 0), __consumer_offsets-48 -> Vector(0, 1, 2), __consumer_offsets-19 -> Vector(1, 2, 0), __consumer_offsets-11 -> Vector(2, 1, 0), __consumer_offsets-13 -> Vector(1, 2, 0), __consumer_offsets-2 -> Vector(2, 0, 1), __consumer_offsets-43 -> Vector(1, 2, 0), __consumer_offsets-6 -> Vector(0, 1, 2), __consumer_offsets-14 -> Vector(2, 0, 1), __consumer_offsets-20 -> Vector(2, 0, 1), __consumer_offsets-0 -> Vector(0, 1, 2), __consumer_offsets-44 -> Vector(2, 0, 1), __consumer_offsets-39 -> Vector(0, 2, 1), __consumer_offsets-12 -> Vector(0, 1, 2), __consumer_offsets-45 -> Vector(0, 2, 1), __consumer_offsets-1 -> Vector(1, 2, 0), __consumer_offsets-5 -> Vector(2, 1, 0), __consumer_offsets-26 -> Vector(2, 0, 1), __consumer_offsets-29 -> Vector(2, 1, 0), __consumer_offsets-34 -> Vector(1, 0, 2), __consumer_offsets-10 -> Vector(1, 0, 2), __consumer_offsets-32 -> Vector(2, 0, 1), __consumer_offsets-40 -> Vector(1, 0, 2))] (kafka.controller.KafkaController)
[2020-05-13 17:33:58,253] INFO [Controller id=0] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController)
[2020-05-13 17:33:58,254] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1,0,2 (state.change.logger)
[2020-05-13 17:33:58,254] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,254] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1 (state.change.logger)
[2020-05-13 17:33:58,254] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 0,2,1 (state.change.logger)
[2020-05-13 17:33:58,254] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1,0,2 (state.change.logger)
[2020-05-13 17:33:58,255] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 0,2,1 (state.change.logger)
[2020-05-13 17:33:58,255] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,257] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 0,2,1 (state.change.logger)
[2020-05-13 17:33:58,257] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 1,0,2 (state.change.logger)
[2020-05-13 17:33:58,258] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,258] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 2,1,0 (state.change.logger)
[2020-05-13 17:33:58,258] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 2,1,0 (state.change.logger)
[2020-05-13 17:33:58,258] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 0,2,1 (state.change.logger)
[2020-05-13 17:33:58,258] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 2,1,0 (state.change.logger)
[2020-05-13 17:33:58,258] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,258] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 2,1,0 (state.change.logger)
[2020-05-13 17:33:58,259] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1,0,2 (state.change.logger)
[2020-05-13 17:33:58,259] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 1,0,2 (state.change.logger)
[2020-05-13 17:33:58,259] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,259] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,259] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,259] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0,2,1 (state.change.logger)
[2020-05-13 17:33:58,259] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,260] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,260] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 0,2,1 (state.change.logger)
[2020-05-13 17:33:58,260] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,260] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1 (state.change.logger)
[2020-05-13 17:33:58,260] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 2,1,0 (state.change.logger)
[2020-05-13 17:33:58,261] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,261] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,262] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 2,1,0 (state.change.logger)
[2020-05-13 17:33:58,262] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,262] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1 (state.change.logger)
[2020-05-13 17:33:58,262] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,262] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,262] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1 (state.change.logger)
[2020-05-13 17:33:58,262] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1 (state.change.logger)
[2020-05-13 17:33:58,263] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,263] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1 (state.change.logger)
[2020-05-13 17:33:58,263] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 0,2,1 (state.change.logger)
[2020-05-13 17:33:58,264] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 0,1,2 (state.change.logger)
[2020-05-13 17:33:58,264] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 0,2,1 (state.change.logger)
[2020-05-13 17:33:58,264] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1,2,0 (state.change.logger)
[2020-05-13 17:33:58,264] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 2,1,0 (state.change.logger)
[2020-05-13 17:33:58,264] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1 (state.change.logger)
[2020-05-13 17:33:58,264] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 2,1,0 (state.change.logger)
[2020-05-13 17:33:58,264] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 1,0,2 (state.change.logger)
[2020-05-13 17:33:58,264] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1,0,2 (state.change.logger)
[2020-05-13 17:33:58,265] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 2,0,1 (state.change.logger)
[2020-05-13 17:33:58,265] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 1,0,2 (state.change.logger)
[2020-05-13 17:33:58,268] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,268] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,269] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,269] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,269] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,269] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,269] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,269] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,269] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,270] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,270] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,270] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,270] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,270] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,270] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,271] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,271] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,271] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,271] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,271] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,271] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,271] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,272] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,273] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,273] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,273] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,273] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,273] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,273] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,273] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,274] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,275] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,275] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,275] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,275] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,275] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,276] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,277] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,278] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,279] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,279] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,279] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,279] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,279] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,279] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,279] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,279] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,280] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,281] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,281] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,281] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,281] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,281] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,281] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,281] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,282] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,282] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,282] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,282] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,282] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,282] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,282] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,283] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,284] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,284] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,284] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,284] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,284] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,284] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,284] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,284] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,285] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,286] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,286] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,286] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,286] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,286] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,286] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,286] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,286] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,287] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,287] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,287] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,287] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:33:58,406] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 0, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,406] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,406] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,406] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 2, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,406] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 0, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,406] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 2, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 2, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 0, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 1, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 1, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 2, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 1, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,407] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,408] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 1, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,408] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 0, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,408] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 0, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,408] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,408] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,408] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,408] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 2, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,414] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 2, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 1, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 1, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,415] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,416] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,416] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,416] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,416] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 2, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,416] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 1, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,416] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0, 2, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,416] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 2, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,416] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 1, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 1, 0), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 0, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 0, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=2, leaderEpoch=0, isr=List(2, 0, 1), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1, 0, 2), zkVersion=0) (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-49 (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 2 for partition __consumer_offsets-38 (state.change.logger)
[2020-05-13 17:33:58,417] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 2 for partition __consumer_offsets-16 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 2 for partition __consumer_offsets-27 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 2 for partition __consumer_offsets-8 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-19 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-13 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 2 for partition __consumer_offsets-2 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 2 for partition __consumer_offsets-46 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 2 for partition __consumer_offsets-35 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-24 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 2 for partition __consumer_offsets-5 (state.change.logger)
[2020-05-13 17:33:58,418] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-43 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 2 for partition __consumer_offsets-21 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 2 for partition __consumer_offsets-32 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 2 for partition __consumer_offsets-10 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-37 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-48 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 2 for partition __consumer_offsets-40 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-18 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 2 for partition __consumer_offsets-29 (state.change.logger)
[2020-05-13 17:33:58,419] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-7 (state.change.logger)
[2020-05-13 17:33:58,420] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 2 for partition __consumer_offsets-23 (state.change.logger)
[2020-05-13 17:33:58,420] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 2 for partition __consumer_offsets-45 (state.change.logger)
[2020-05-13 17:33:58,420] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 2 for partition __consumer_offsets-34 (state.change.logger)
[2020-05-13 17:33:58,420] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 2 for partition __consumer_offsets-26 (state.change.logger)
[2020-05-13 17:33:58,420] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 2 for partition __consumer_offsets-15 (state.change.logger)
[2020-05-13 17:33:58,420] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 2 for partition __consumer_offsets-4 (state.change.logger)
[2020-05-13 17:33:58,420] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-42 (state.change.logger)
[2020-05-13 17:33:58,420] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-31 (state.change.logger)
[2020-05-13 17:33:58,421] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 2 for partition __consumer_offsets-9 (state.change.logger)
[2020-05-13 17:33:58,421] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 2 for partition __consumer_offsets-20 (state.change.logger)
[2020-05-13 17:33:58,421] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-12 (state.change.logger)
[2020-05-13 17:33:58,421] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-1 (state.change.logger)
[2020-05-13 17:33:58,421] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 2 for partition __consumer_offsets-28 (state.change.logger)
[2020-05-13 17:33:58,421] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 2 for partition __consumer_offsets-17 (state.change.logger)
[2020-05-13 17:33:58,421] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-6 (state.change.logger)
[2020-05-13 17:33:58,421] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 2 for partition __consumer_offsets-39 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 2 for partition __consumer_offsets-44 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-36 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 2 for partition __consumer_offsets-47 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 2 for partition __consumer_offsets-3 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 2 for partition __consumer_offsets-25 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 2 for partition __consumer_offsets-14 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-30 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 2 for partition __consumer_offsets-41 (state.change.logger)
[2020-05-13 17:33:58,422] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 2 for partition __consumer_offsets-22 (state.change.logger)
[2020-05-13 17:33:58,423] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 2 for partition __consumer_offsets-33 (state.change.logger)
[2020-05-13 17:33:58,423] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 2 for partition __consumer_offsets-11 (state.change.logger)
[2020-05-13 17:33:58,423] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 2 for partition __consumer_offsets-0 (state.change.logger)
[2020-05-13 17:33:58,423] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-49 (state.change.logger)
[2020-05-13 17:33:58,423] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 1 for partition __consumer_offsets-38 (state.change.logger)
[2020-05-13 17:33:58,424] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 1 for partition __consumer_offsets-16 (state.change.logger)
[2020-05-13 17:33:58,424] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 1 for partition __consumer_offsets-27 (state.change.logger)
[2020-05-13 17:33:58,424] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 1 for partition __consumer_offsets-8 (state.change.logger)
[2020-05-13 17:33:58,424] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-19 (state.change.logger)
[2020-05-13 17:33:58,426] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-13 (state.change.logger)
[2020-05-13 17:33:58,426] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 1 for partition __consumer_offsets-2 (state.change.logger)
[2020-05-13 17:33:58,426] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 1 for partition __consumer_offsets-46 (state.change.logger)
[2020-05-13 17:33:58,426] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 1 for partition __consumer_offsets-35 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-24 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 1 for partition __consumer_offsets-5 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-43 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 1 for partition __consumer_offsets-21 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 1 for partition __consumer_offsets-32 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 1 for partition __consumer_offsets-10 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-37 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-48 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 1 for partition __consumer_offsets-40 (state.change.logger)
[2020-05-13 17:33:58,427] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-18 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 1 for partition __consumer_offsets-29 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-7 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 1 for partition __consumer_offsets-23 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 1 for partition __consumer_offsets-45 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 1 for partition __consumer_offsets-34 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 1 for partition __consumer_offsets-26 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 1 for partition __consumer_offsets-15 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 1 for partition __consumer_offsets-4 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-42 (state.change.logger)
[2020-05-13 17:33:58,428] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-31 (state.change.logger)
[2020-05-13 17:33:58,429] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 1 for partition __consumer_offsets-9 (state.change.logger)
[2020-05-13 17:33:58,429] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 1 for partition __consumer_offsets-20 (state.change.logger)
[2020-05-13 17:33:58,429] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-12 (state.change.logger)
[2020-05-13 17:33:58,429] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-1 (state.change.logger)
[2020-05-13 17:33:58,429] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 1 for partition __consumer_offsets-28 (state.change.logger)
[2020-05-13 17:33:58,429] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 1 for partition __consumer_offsets-17 (state.change.logger)
[2020-05-13 17:33:58,429] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-6 (state.change.logger)
[2020-05-13 17:33:58,432] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 1 for partition __consumer_offsets-39 (state.change.logger)
[2020-05-13 17:33:58,432] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 1 for partition __consumer_offsets-44 (state.change.logger)
[2020-05-13 17:33:58,434] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-36 (state.change.logger)
[2020-05-13 17:33:58,434] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 1 for partition __consumer_offsets-47 (state.change.logger)
[2020-05-13 17:33:58,434] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 1 for partition __consumer_offsets-3 (state.change.logger)
[2020-05-13 17:33:58,434] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 1 for partition __consumer_offsets-25 (state.change.logger)
[2020-05-13 17:33:58,434] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 1 for partition __consumer_offsets-14 (state.change.logger)
[2020-05-13 17:33:58,434] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-30 (state.change.logger)
[2020-05-13 17:33:58,434] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 1 for partition __consumer_offsets-41 (state.change.logger)
[2020-05-13 17:33:58,434] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 1 for partition __consumer_offsets-22 (state.change.logger)
[2020-05-13 17:33:58,435] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 1 for partition __consumer_offsets-33 (state.change.logger)
[2020-05-13 17:33:58,435] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 1 for partition __consumer_offsets-11 (state.change.logger)
[2020-05-13 17:33:58,435] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 1 for partition __consumer_offsets-0 (state.change.logger)
[2020-05-13 17:33:58,437] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-49 (state.change.logger)
[2020-05-13 17:33:58,437] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 0 for partition __consumer_offsets-38 (state.change.logger)
[2020-05-13 17:33:58,437] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 0 for partition __consumer_offsets-16 (state.change.logger)
[2020-05-13 17:33:58,437] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 0 for partition __consumer_offsets-27 (state.change.logger)
[2020-05-13 17:33:58,437] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 0 for partition __consumer_offsets-8 (state.change.logger)
[2020-05-13 17:33:58,437] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-19 (state.change.logger)
[2020-05-13 17:33:58,438] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-13 (state.change.logger)
[2020-05-13 17:33:58,438] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 0 for partition __consumer_offsets-2 (state.change.logger)
[2020-05-13 17:33:58,438] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 0 for partition __consumer_offsets-46 (state.change.logger)
[2020-05-13 17:33:58,438] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 0 for partition __consumer_offsets-35 (state.change.logger)
[2020-05-13 17:33:58,438] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-24 (state.change.logger)
[2020-05-13 17:33:58,438] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 0 for partition __consumer_offsets-5 (state.change.logger)
[2020-05-13 17:33:58,438] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-43 (state.change.logger)
[2020-05-13 17:33:58,438] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 0 for partition __consumer_offsets-21 (state.change.logger)
[2020-05-13 17:33:58,439] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 0 for partition __consumer_offsets-32 (state.change.logger)
[2020-05-13 17:33:58,439] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 0 for partition __consumer_offsets-10 (state.change.logger)
[2020-05-13 17:33:58,439] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-37 (state.change.logger)
[2020-05-13 17:33:58,439] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-48 (state.change.logger)
[2020-05-13 17:33:58,440] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 0 for partition __consumer_offsets-40 (state.change.logger)
[2020-05-13 17:33:58,440] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-18 (state.change.logger)
[2020-05-13 17:33:58,440] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 0 for partition __consumer_offsets-29 (state.change.logger)
[2020-05-13 17:33:58,441] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-7 (state.change.logger)
[2020-05-13 17:33:58,441] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 0 for partition __consumer_offsets-23 (state.change.logger)
[2020-05-13 17:33:58,442] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 0 for partition __consumer_offsets-45 (state.change.logger)
[2020-05-13 17:33:58,442] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 0 for partition __consumer_offsets-34 (state.change.logger)
[2020-05-13 17:33:58,442] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 0 for partition __consumer_offsets-26 (state.change.logger)
[2020-05-13 17:33:58,442] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 0 for partition __consumer_offsets-15 (state.change.logger)
[2020-05-13 17:33:58,442] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 0 for partition __consumer_offsets-4 (state.change.logger)
[2020-05-13 17:33:58,442] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-42 (state.change.logger)
[2020-05-13 17:33:58,442] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-31 (state.change.logger)
[2020-05-13 17:33:58,442] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 0 for partition __consumer_offsets-9 (state.change.logger)
[2020-05-13 17:33:58,443] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 0 for partition __consumer_offsets-20 (state.change.logger)
[2020-05-13 17:33:58,443] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-12 (state.change.logger)
[2020-05-13 17:33:58,443] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-1 (state.change.logger)
[2020-05-13 17:33:58,443] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 0 for partition __consumer_offsets-28 (state.change.logger)
[2020-05-13 17:33:58,443] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 0 for partition __consumer_offsets-17 (state.change.logger)
[2020-05-13 17:33:58,443] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-6 (state.change.logger)
[2020-05-13 17:33:58,443] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 0 for partition __consumer_offsets-39 (state.change.logger)
[2020-05-13 17:33:58,443] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 0 for partition __consumer_offsets-44 (state.change.logger)
[2020-05-13 17:33:58,444] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-36 (state.change.logger)
[2020-05-13 17:33:58,444] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 0 for partition __consumer_offsets-47 (state.change.logger)
[2020-05-13 17:33:58,444] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 0 for partition __consumer_offsets-3 (state.change.logger)
[2020-05-13 17:33:58,445] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) to broker 0 for partition __consumer_offsets-25 (state.change.logger)
[2020-05-13 17:33:58,445] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) to broker 0 for partition __consumer_offsets-14 (state.change.logger)
[2020-05-13 17:33:58,446] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-30 (state.change.logger)
[2020-05-13 17:33:58,446] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 0 for partition __consumer_offsets-41 (state.change.logger)
[2020-05-13 17:33:58,447] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) to broker 0 for partition __consumer_offsets-22 (state.change.logger)
[2020-05-13 17:33:58,447] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) to broker 0 for partition __consumer_offsets-33 (state.change.logger)
[2020-05-13 17:33:58,447] TRACE [Controller id=0 epoch=1] Sending become-follower LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) to broker 0 for partition __consumer_offsets-11 (state.change.logger)
[2020-05-13 17:33:58,447] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) to broker 0 for partition __consumer_offsets-0 (state.change.logger)
[2020-05-13 17:33:58,450] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-13 (state.change.logger)
[2020-05-13 17:33:58,450] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-46 (state.change.logger)
[2020-05-13 17:33:58,450] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-9 (state.change.logger)
[2020-05-13 17:33:58,450] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-42 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-21 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-17 (state.change.logger)
[2020-05-13 17:33:58,450] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-49 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-30 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-26 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-5 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-38 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-1 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-34 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-38 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-16 (state.change.logger)
[2020-05-13 17:33:58,451] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-45 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-12 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-16 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-41 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-24 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-20 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-49 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-0 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-29 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-25 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-8 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-37 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-4 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-33 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-15 (state.change.logger)
[2020-05-13 17:33:58,452] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-48 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-11 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-44 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-23 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-19 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-32 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-27 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-28 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-7 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-40 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-3 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-36 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-47 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-14 (state.change.logger)
[2020-05-13 17:33:58,453] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-43 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-8 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-10 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,0,2, zkVersion=0, replicas=1,0,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-22 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-18 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1,2,0, zkVersion=0, replicas=1,2,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-31 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-27 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,2,1, zkVersion=0, replicas=0,2,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-39 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=0,1,2, zkVersion=0, replicas=0,1,2, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-6 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,1,0, zkVersion=0, replicas=2,1,0, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-35 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Broker id=0] Received LeaderAndIsr request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=2,0,1, zkVersion=0, replicas=2,0,1, isNew=true) correlation id 7 from controller 0 epoch 1 for partition __consumer_offsets-2 (state.change.logger)
[2020-05-13 17:33:58,454] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-19 (state.change.logger)
[2020-05-13 17:33:58,456] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-13 (state.change.logger)
[2020-05-13 17:33:58,456] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-2 (state.change.logger)
[2020-05-13 17:33:58,456] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-46 (state.change.logger)
[2020-05-13 17:33:58,456] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-35 (state.change.logger)
[2020-05-13 17:33:58,456] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-24 (state.change.logger)
[2020-05-13 17:33:58,456] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-5 (state.change.logger)
[2020-05-13 17:33:58,457] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-43 (state.change.logger)
[2020-05-13 17:33:58,457] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-21 (state.change.logger)
[2020-05-13 17:33:58,457] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-32 (state.change.logger)
[2020-05-13 17:33:58,457] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-10 (state.change.logger)
[2020-05-13 17:33:58,457] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-37 (state.change.logger)
[2020-05-13 17:33:58,457] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-48 (state.change.logger)
[2020-05-13 17:33:58,457] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-40 (state.change.logger)
[2020-05-13 17:33:58,457] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-18 (state.change.logger)
[2020-05-13 17:33:58,458] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-29 (state.change.logger)
[2020-05-13 17:33:58,458] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-7 (state.change.logger)
[2020-05-13 17:33:58,458] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-23 (state.change.logger)
[2020-05-13 17:33:58,458] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-45 (state.change.logger)
[2020-05-13 17:33:58,459] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-34 (state.change.logger)
[2020-05-13 17:33:58,459] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-26 (state.change.logger)
[2020-05-13 17:33:58,459] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-15 (state.change.logger)
[2020-05-13 17:33:58,459] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-4 (state.change.logger)
[2020-05-13 17:33:58,459] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-42 (state.change.logger)
[2020-05-13 17:33:58,460] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-31 (state.change.logger)
[2020-05-13 17:33:58,460] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-9 (state.change.logger)
[2020-05-13 17:33:58,460] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-20 (state.change.logger)
[2020-05-13 17:33:58,460] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-12 (state.change.logger)
[2020-05-13 17:33:58,461] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-1 (state.change.logger)
[2020-05-13 17:33:58,461] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-28 (state.change.logger)
[2020-05-13 17:33:58,461] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-17 (state.change.logger)
[2020-05-13 17:33:58,461] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-6 (state.change.logger)
[2020-05-13 17:33:58,461] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-39 (state.change.logger)
[2020-05-13 17:33:58,462] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 0 sent to broker 100.96.4.12:9092 (id: 2 rack: null) (state.change.logger)
[2020-05-13 17:33:58,465] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-44 (state.change.logger)
[2020-05-13 17:33:58,465] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-36 (state.change.logger)
[2020-05-13 17:33:58,466] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-47 (state.change.logger)
[2020-05-13 17:33:58,466] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-3 (state.change.logger)
[2020-05-13 17:33:58,466] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-25 (state.change.logger)
[2020-05-13 17:33:58,466] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-14 (state.change.logger)
[2020-05-13 17:33:58,466] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-30 (state.change.logger)
[2020-05-13 17:33:58,466] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-41 (state.change.logger)
[2020-05-13 17:33:58,467] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-22 (state.change.logger)
[2020-05-13 17:33:58,468] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-33 (state.change.logger)
[2020-05-13 17:33:58,468] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-11 (state.change.logger)
[2020-05-13 17:33:58,468] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition __consumer_offsets-0 (state.change.logger)
[2020-05-13 17:33:58,472] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,472] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,472] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,472] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,472] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,472] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,472] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,473] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,473] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,473] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,473] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,473] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,473] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,473] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,474] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,475] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,475] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,476] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,476] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,476] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,476] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,476] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,476] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,476] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,477] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,477] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,477] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,477] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,477] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,477] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,477] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,477] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,478] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,478] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,478] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,478] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,478] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,478] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,478] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,478] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,479] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,479] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,479] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,479] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,481] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,481] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,482] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,482] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,482] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,482] TRACE [Controller id=0 epoch=1] Changed state of replica 2 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,483] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,483] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,483] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,483] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,483] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,483] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,483] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,484] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,484] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,484] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,484] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,484] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,484] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,484] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,484] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,485] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,485] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,485] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,485] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,485] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,487] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,487] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,488] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,488] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,488] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,488] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,488] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,488] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,488] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,489] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,489] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,489] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,489] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,489] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,489] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,490] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,490] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,490] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,490] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,490] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,490] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,490] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,490] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,491] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,491] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,491] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,491] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,491] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,491] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,491] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,492] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,492] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,492] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,493] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,493] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,493] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,493] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,495] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,495] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,496] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,496] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,496] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,496] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,496] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,496] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,496] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,497] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,497] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,497] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,497] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,497] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,497] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,498] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,498] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,498] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,498] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,498] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,498] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,498] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,499] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,499] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,499] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,499] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,499] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,499] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,499] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,499] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,500] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,500] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,500] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,500] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,500] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,501] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,501] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,501] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,501] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,501] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,502] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,503] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,503] TRACE [Controller id=0 epoch=1] Changed state of replica 0 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:33:58,511] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
[2020-05-13 17:33:58,511] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
[2020-05-13 17:33:58,511] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
[2020-05-13 17:33:58,511] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
[2020-05-13 17:33:58,511] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
[2020-05-13 17:33:58,512] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
[2020-05-13 17:33:58,512] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-30,__consumer_offsets-21,__consumer_offsets-27,__consumer_offsets-9,__consumer_offsets-33,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-48,__consumer_offsets-6,__consumer_offsets-0,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45 (kafka.server.ReplicaFetcherManager)
[2020-05-13 17:33:58,522] INFO [Log partition=__consumer_offsets-0, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,524] INFO [Log partition=__consumer_offsets-0, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2020-05-13 17:33:58,525] INFO Created log for partition __consumer_offsets-0 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,525] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2020-05-13 17:33:58,526] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,526] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,526] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,527] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,531] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-0 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,541] INFO [Log partition=__consumer_offsets-48, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,542] INFO [Log partition=__consumer_offsets-48, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,543] INFO Created log for partition __consumer_offsets-48 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,543] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2020-05-13 17:33:58,543] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,544] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,544] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,544] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,547] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-48 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,552] INFO [Log partition=__consumer_offsets-45, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,555] INFO [Log partition=__consumer_offsets-45, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2020-05-13 17:33:58,556] INFO Created log for partition __consumer_offsets-45 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,556] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2020-05-13 17:33:58,556] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,556] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,556] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,556] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,559] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-45 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,562] INFO [Log partition=__consumer_offsets-42, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,562] INFO [Log partition=__consumer_offsets-42, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,585] INFO Created log for partition __consumer_offsets-42 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,585] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2020-05-13 17:33:58,585] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,585] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,585] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,585] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,588] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-42 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,594] INFO [Log partition=__consumer_offsets-39, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,595] INFO [Log partition=__consumer_offsets-39, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,596] INFO Created log for partition __consumer_offsets-39 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,597] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2020-05-13 17:33:58,597] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,597] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,597] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,597] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,600] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-39 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,604] INFO [Log partition=__consumer_offsets-36, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,605] INFO [Log partition=__consumer_offsets-36, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2020-05-13 17:33:58,606] INFO Created log for partition __consumer_offsets-36 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,606] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2020-05-13 17:33:58,606] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,606] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,606] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,606] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,609] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-36 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,614] INFO [Log partition=__consumer_offsets-33, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,614] INFO [Log partition=__consumer_offsets-33, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,615] INFO Created log for partition __consumer_offsets-33 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,615] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2020-05-13 17:33:58,615] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,616] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,616] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,616] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,618] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-33 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,622] INFO [Log partition=__consumer_offsets-30, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,623] INFO [Log partition=__consumer_offsets-30, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,623] INFO Created log for partition __consumer_offsets-30 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,623] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2020-05-13 17:33:58,623] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,624] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,624] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,624] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,626] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-30 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,630] INFO [Log partition=__consumer_offsets-27, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,631] INFO [Log partition=__consumer_offsets-27, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,631] INFO Created log for partition __consumer_offsets-27 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,632] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2020-05-13 17:33:58,632] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,632] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,632] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,633] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,635] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-27 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,638] INFO [Log partition=__consumer_offsets-24, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,639] INFO [Log partition=__consumer_offsets-24, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,640] INFO Created log for partition __consumer_offsets-24 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,640] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2020-05-13 17:33:58,640] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,640] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,640] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,640] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,643] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-24 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,647] INFO [Log partition=__consumer_offsets-21, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,647] INFO [Log partition=__consumer_offsets-21, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,648] INFO Created log for partition __consumer_offsets-21 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,648] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2020-05-13 17:33:58,648] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,648] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,648] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,649] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,651] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-21 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,654] INFO [Log partition=__consumer_offsets-18, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,654] INFO [Log partition=__consumer_offsets-18, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,655] INFO Created log for partition __consumer_offsets-18 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,655] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2020-05-13 17:33:58,655] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,655] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,655] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,655] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,658] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-18 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,661] INFO [Log partition=__consumer_offsets-15, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,662] INFO [Log partition=__consumer_offsets-15, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,663] INFO Created log for partition __consumer_offsets-15 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,663] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2020-05-13 17:33:58,663] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,663] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,663] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,663] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,666] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-15 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,668] INFO [Log partition=__consumer_offsets-12, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,669] INFO [Log partition=__consumer_offsets-12, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,669] INFO Created log for partition __consumer_offsets-12 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,671] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2020-05-13 17:33:58,671] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,671] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,671] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,671] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,673] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-12 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,677] INFO [Log partition=__consumer_offsets-9, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,677] INFO [Log partition=__consumer_offsets-9, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,678] INFO Created log for partition __consumer_offsets-9 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,680] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2020-05-13 17:33:58,680] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,680] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,680] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,680] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,682] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-9 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,689] INFO [Log partition=__consumer_offsets-6, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,690] INFO [Log partition=__consumer_offsets-6, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,691] INFO Created log for partition __consumer_offsets-6 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,691] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2020-05-13 17:33:58,691] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,691] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,691] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,691] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,694] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-6 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,698] INFO [Log partition=__consumer_offsets-3, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,701] INFO [Log partition=__consumer_offsets-3, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2020-05-13 17:33:58,702] INFO Created log for partition __consumer_offsets-3 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,703] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2020-05-13 17:33:58,703] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,703] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,704] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,704] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,706] TRACE [Broker id=0] Stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-3 (last update controller epoch 1) (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
[2020-05-13 17:33:58,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-29 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-10 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-26 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-7 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-4 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-23 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-20 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-17 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-14 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-49 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-11 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-46 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-8 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-43 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-5 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-2 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-40 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,708] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-37 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-34 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-31 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-47 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-19 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-28 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-38 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-35 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-44 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-25 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-16 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-22 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-41 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-32 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,709] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-follower transition for partition __consumer_offsets-13 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,712] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,712] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,715] INFO [Log partition=__consumer_offsets-29, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,716] INFO [Log partition=__consumer_offsets-29, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2020-05-13 17:33:58,718] INFO Created log for partition __consumer_offsets-29 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,718] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2020-05-13 17:33:58,718] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,719] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,723] INFO [Log partition=__consumer_offsets-10, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,724] INFO [Log partition=__consumer_offsets-10, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2020-05-13 17:33:58,725] INFO Created log for partition __consumer_offsets-10 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,725] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2020-05-13 17:33:58,725] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,725] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,726] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,729] INFO [Log partition=__consumer_offsets-26, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,730] INFO [Log partition=__consumer_offsets-26, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,732] INFO Created log for partition __consumer_offsets-26 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,732] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2020-05-13 17:33:58,733] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,733] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,733] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,733] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,737] INFO [Log partition=__consumer_offsets-7, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,739] INFO [Log partition=__consumer_offsets-7, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2020-05-13 17:33:58,739] INFO Created log for partition __consumer_offsets-7 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,740] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2020-05-13 17:33:58,740] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,740] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,743] INFO [Log partition=__consumer_offsets-4, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,744] INFO [Log partition=__consumer_offsets-4, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2020-05-13 17:33:58,745] INFO Created log for partition __consumer_offsets-4 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,745] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2020-05-13 17:33:58,745] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,745] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,746] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,746] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,751] INFO [Log partition=__consumer_offsets-23, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,751] INFO [Log partition=__consumer_offsets-23, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2020-05-13 17:33:58,752] INFO Created log for partition __consumer_offsets-23 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,757] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2020-05-13 17:33:58,757] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,757] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,757] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,760] INFO [Log partition=__consumer_offsets-1, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,761] INFO [Log partition=__consumer_offsets-1, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,761] INFO Created log for partition __consumer_offsets-1 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,762] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2020-05-13 17:33:58,762] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,762] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,770] INFO [Log partition=__consumer_offsets-20, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,771] INFO [Log partition=__consumer_offsets-20, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2020-05-13 17:33:58,772] INFO Created log for partition __consumer_offsets-20 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,779] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2020-05-13 17:33:58,779] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,779] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,779] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,779] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,782] INFO [Log partition=__consumer_offsets-17, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,782] INFO [Log partition=__consumer_offsets-17, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,783] INFO Created log for partition __consumer_offsets-17 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,783] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2020-05-13 17:33:58,783] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,784] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,787] INFO [Log partition=__consumer_offsets-14, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,787] INFO [Log partition=__consumer_offsets-14, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,788] INFO Created log for partition __consumer_offsets-14 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,788] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2020-05-13 17:33:58,788] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,788] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,789] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,789] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,795] INFO [Log partition=__consumer_offsets-49, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,795] INFO [Log partition=__consumer_offsets-49, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,796] INFO Created log for partition __consumer_offsets-49 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,796] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2020-05-13 17:33:58,796] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,796] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,796] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,801] INFO [Log partition=__consumer_offsets-11, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,801] INFO [Log partition=__consumer_offsets-11, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,802] INFO Created log for partition __consumer_offsets-11 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,802] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2020-05-13 17:33:58,802] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,802] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,805] INFO [Log partition=__consumer_offsets-46, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,805] INFO [Log partition=__consumer_offsets-46, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,806] INFO Created log for partition __consumer_offsets-46 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,806] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2020-05-13 17:33:58,806] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,806] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,807] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,809] INFO [Log partition=__consumer_offsets-8, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,809] INFO [Log partition=__consumer_offsets-8, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,810] INFO Created log for partition __consumer_offsets-8 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,810] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2020-05-13 17:33:58,810] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,810] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,811] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,811] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,813] INFO [Log partition=__consumer_offsets-43, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,813] INFO [Log partition=__consumer_offsets-43, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,819] INFO Created log for partition __consumer_offsets-43 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,820] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2020-05-13 17:33:58,820] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,820] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,820] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,823] INFO [Log partition=__consumer_offsets-5, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,824] INFO [Log partition=__consumer_offsets-5, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2020-05-13 17:33:58,824] INFO Created log for partition __consumer_offsets-5 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,825] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2020-05-13 17:33:58,825] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,825] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,839] INFO [Log partition=__consumer_offsets-2, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,840] INFO [Log partition=__consumer_offsets-2, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,840] INFO Created log for partition __consumer_offsets-2 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,840] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2020-05-13 17:33:58,840] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,840] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,841] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,844] INFO [Log partition=__consumer_offsets-40, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,844] INFO [Log partition=__consumer_offsets-40, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,845] INFO Created log for partition __consumer_offsets-40 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,845] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2020-05-13 17:33:58,845] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,845] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,846] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,846] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,848] INFO [Log partition=__consumer_offsets-37, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,848] INFO [Log partition=__consumer_offsets-37, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,849] INFO Created log for partition __consumer_offsets-37 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,849] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2020-05-13 17:33:58,849] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,849] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,857] INFO [Log partition=__consumer_offsets-34, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,858] INFO [Log partition=__consumer_offsets-34, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,858] INFO Created log for partition __consumer_offsets-34 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,858] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2020-05-13 17:33:58,858] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,858] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,858] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,858] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,861] INFO [Log partition=__consumer_offsets-31, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,862] INFO [Log partition=__consumer_offsets-31, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,862] INFO Created log for partition __consumer_offsets-31 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,862] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2020-05-13 17:33:58,862] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,862] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,862] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,867] INFO [Log partition=__consumer_offsets-47, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,867] INFO [Log partition=__consumer_offsets-47, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 0 ms (kafka.log.Log)
[2020-05-13 17:33:58,868] INFO Created log for partition __consumer_offsets-47 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,868] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2020-05-13 17:33:58,868] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,868] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,868] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,875] INFO [Log partition=__consumer_offsets-19, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,876] INFO [Log partition=__consumer_offsets-19, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,876] INFO Created log for partition __consumer_offsets-19 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,877] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2020-05-13 17:33:58,877] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,877] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,882] INFO [Log partition=__consumer_offsets-28, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,883] INFO [Log partition=__consumer_offsets-28, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,883] INFO Created log for partition __consumer_offsets-28 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,883] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2020-05-13 17:33:58,883] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,884] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,884] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,887] INFO [Log partition=__consumer_offsets-38, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,887] INFO [Log partition=__consumer_offsets-38, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,888] INFO Created log for partition __consumer_offsets-38 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,889] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2020-05-13 17:33:58,889] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,889] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,889] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,889] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,893] INFO [Log partition=__consumer_offsets-35, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,894] INFO [Log partition=__consumer_offsets-35, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,894] INFO Created log for partition __consumer_offsets-35 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,895] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2020-05-13 17:33:58,895] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,895] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,900] INFO [Log partition=__consumer_offsets-44, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,900] INFO [Log partition=__consumer_offsets-44, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,901] INFO Created log for partition __consumer_offsets-44 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,901] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2020-05-13 17:33:58,901] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,901] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,902] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,902] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,904] INFO [Log partition=__consumer_offsets-25, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,905] INFO [Log partition=__consumer_offsets-25, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,905] INFO Created log for partition __consumer_offsets-25 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,906] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2020-05-13 17:33:58,906] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,906] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,913] INFO [Log partition=__consumer_offsets-16, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,914] INFO [Log partition=__consumer_offsets-16, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2020-05-13 17:33:58,915] INFO Created log for partition __consumer_offsets-16 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,916] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2020-05-13 17:33:58,916] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,916] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,917] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,927] INFO [Log partition=__consumer_offsets-22, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,928] INFO [Log partition=__consumer_offsets-22, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2020-05-13 17:33:58,929] INFO Created log for partition __consumer_offsets-22 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,929] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2020-05-13 17:33:58,929] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,929] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,930] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,930] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,934] INFO [Log partition=__consumer_offsets-41, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,935] INFO [Log partition=__consumer_offsets-41, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2020-05-13 17:33:58,935] INFO Created log for partition __consumer_offsets-41 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,936] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2020-05-13 17:33:58,936] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,936] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,945] INFO [Log partition=__consumer_offsets-32, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,945] INFO [Log partition=__consumer_offsets-32, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,946] INFO Created log for partition __consumer_offsets-32 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,946] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2020-05-13 17:33:58,946] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,946] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,947] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,947] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,955] INFO [Log partition=__consumer_offsets-13, dir=/opt/kafka/data/logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-05-13 17:33:58,955] INFO [Log partition=__consumer_offsets-13, dir=/opt/kafka/data/logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2020-05-13 17:33:58,956] INFO Created log for partition __consumer_offsets-13 in /opt/kafka/data/logs with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2020-05-13 17:33:58,956] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2020-05-13 17:33:58,956] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2020-05-13 17:33:58,957] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-28,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-14,__consumer_offsets-40,__consumer_offsets-37,__consumer_offsets-22,__consumer_offsets-41,__consumer_offsets-4,__consumer_offsets-23,__consumer_offsets-26,__consumer_offsets-8,__consumer_offsets-49,__consumer_offsets-31,__consumer_offsets-13,__consumer_offsets-35,__consumer_offsets-17,__consumer_offsets-43,__consumer_offsets-25,__consumer_offsets-44,__consumer_offsets-47,__consumer_offsets-7,__consumer_offsets-29,__consumer_offsets-11,__consumer_offsets-34,__consumer_offsets-19,__consumer_offsets-16,__consumer_offsets-38,__consumer_offsets-1,__consumer_offsets-20,__consumer_offsets-5,__consumer_offsets-46,__consumer_offsets-2 (kafka.server.ReplicaFetcherManager)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-22 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-25 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-28 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-31 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-34 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-37 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-40 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-43 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-46 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-49 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-41 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-44 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-47 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,958] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-4 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-7 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-10 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-13 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-16 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-19 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-2 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-5 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-8 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-11 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-14 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-17 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-20 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-23 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-26 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-29 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-32 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-35 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,959] TRACE [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-38 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,961] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-22 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,961] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-25 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,961] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-28 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-31 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-34 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-37 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-40 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-43 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-46 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-49 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-41 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-44 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,966] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-47 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-1 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-4 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-7 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-10 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-13 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-16 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-19 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 1 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-2 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-5 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-8 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-11 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-14 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-17 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-20 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,967] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-23 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,968] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-26 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,968] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-29 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,968] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-32 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,968] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-35 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:58,968] TRACE [Broker id=0] Truncated logs and checkpointed recovery boundaries for partition __consumer_offsets-38 as part of become-follower request with correlation id 7 from controller 0 epoch 1 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,010] INFO [ReplicaFetcherManager on broker 0] Added fetcher for partitions List([__consumer_offsets-22, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-8, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-4, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-7, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-46, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-25, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-35, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-41, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-23, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-49, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-47, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-16, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-28, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-31, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-37, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-38, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-17, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-19, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-11, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-13, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-2, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-43, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-14, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-20, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-44, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-1, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-5, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-26, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-29, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-34, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-10, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] , [__consumer_offsets-32, initOffset 0 to broker BrokerEndPoint(2,100.96.4.12,9092)] , [__consumer_offsets-40, initOffset 0 to broker BrokerEndPoint(1,100.96.5.7,9092)] ) (kafka.server.ReplicaFetcherManager)
[2020-05-13 17:33:59,015] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-22 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,017] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-25 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,017] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-28 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,014] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,012] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partitions=[{topic=__consumer_offsets,partition=13,error_code=0},{topic=__consumer_offsets,partition=46,error_code=0},{topic=__consumer_offsets,partition=9,error_code=0},{topic=__consumer_offsets,partition=42,error_code=0},{topic=__consumer_offsets,partition=21,error_code=0},{topic=__consumer_offsets,partition=17,error_code=0},{topic=__consumer_offsets,partition=30,error_code=0},{topic=__consumer_offsets,partition=26,error_code=0},{topic=__consumer_offsets,partition=5,error_code=0},{topic=__consumer_offsets,partition=38,error_code=0},{topic=__consumer_offsets,partition=1,error_code=0},{topic=__consumer_offsets,partition=34,error_code=0},{topic=__consumer_offsets,partition=16,error_code=0},{topic=__consumer_offsets,partition=45,error_code=0},{topic=__consumer_offsets,partition=12,error_code=0},{topic=__consumer_offsets,partition=41,error_code=0},{topic=__consumer_offsets,partition=24,error_code=0},{topic=__consumer_offsets,partition=20,error_code=0},{topic=__consumer_offsets,partition=49,error_code=0},{topic=__consumer_offsets,partition=0,error_code=0},{topic=__consumer_offsets,partition=29,error_code=0},{topic=__consumer_offsets,partition=25,error_code=0},{topic=__consumer_offsets,partition=8,error_code=0},{topic=__consumer_offsets,partition=37,error_code=0},{topic=__consumer_offsets,partition=4,error_code=0},{topic=__consumer_offsets,partition=33,error_code=0},{topic=__consumer_offsets,partition=15,error_code=0},{topic=__consumer_offsets,partition=48,error_code=0},{topic=__consumer_offsets,partition=11,error_code=0},{topic=__consumer_offsets,partition=44,error_code=0},{topic=__consumer_offsets,partition=23,error_code=0},{topic=__consumer_offsets,partition=19,error_code=0},{topic=__consumer_offsets,partition=32,error_code=0},{topic=__consumer_offsets,partition=28,error_code=0},{topic=__consumer_offsets,partition=7,error_code=0},{topic=__consumer_offsets,partition=40,error_code=0},{topic=__consumer_offsets,partition=3,error_code=0},{topic=__consumer_offsets,partition=36,error_code=0},{topic=__consumer_offsets,partition=47,error_code=0},{topic=__consumer_offsets,partition=14,error_code=0},{topic=__consumer_offsets,partition=43,error_code=0},{topic=__consumer_offsets,partition=10,error_code=0},{topic=__consumer_offsets,partition=22,error_code=0},{topic=__consumer_offsets,partition=18,error_code=0},{topic=__consumer_offsets,partition=31,error_code=0},{topic=__consumer_offsets,partition=27,error_code=0},{topic=__consumer_offsets,partition=39,error_code=0},{topic=__consumer_offsets,partition=6,error_code=0},{topic=__consumer_offsets,partition=35,error_code=0},{topic=__consumer_offsets,partition=2,error_code=0}]} for request LEADER_AND_ISR with correlation id 7 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:59,017] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,026] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-31 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,027] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-34 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,028] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-37 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,028] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-40 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,028] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-43 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,028] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-46 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,028] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-49 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,028] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 8 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:33:59,038] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-41 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,038] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-44 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,038] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-47 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,038] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-1 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,038] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-4 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,038] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-7 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,048] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-10 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,056] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-13 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,056] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-16 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,056] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-19 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,056] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-2 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,056] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-5 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,056] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-8 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,056] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-11 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,056] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-14 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,057] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-17 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,057] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-20 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,057] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-23 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,057] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-26 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,057] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-29 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,057] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-32 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,057] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-35 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,057] TRACE [Broker id=0] Started fetcher to new leader as part of become-follower request from controller 0 epoch 1 with correlation id 7 for partition __consumer_offsets-38 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,058] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-29 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,058] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-10 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,058] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-26 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,058] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-7 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,058] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-4 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-23 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-1 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-20 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-17 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-14 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-49 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-11 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-46 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-8 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-43 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-5 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,059] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-2 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-40 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-37 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-34 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-31 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-47 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-19 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-28 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-38 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-35 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-44 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-25 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,060] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-16 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,061] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-22 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,061] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-41 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,061] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-32 with leader 2 (state.change.logger)
[2020-05-13 17:33:59,061] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-follower transition for partition __consumer_offsets-13 with leader 1 (state.change.logger)
[2020-05-13 17:33:59,061] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2020-05-13 17:33:59,075] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-47. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,083] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-16. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,084] INFO [Log partition=__consumer_offsets-16, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,092] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-13. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,092] INFO [Log partition=__consumer_offsets-13, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,092] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-46. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,092] INFO [Log partition=__consumer_offsets-46, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,092] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-43. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,092] INFO [Log partition=__consumer_offsets-43, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,092] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-10. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,092] INFO [Log partition=__consumer_offsets-10, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,092] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-22. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,092] INFO [Log partition=__consumer_offsets-22, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,092] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-19. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,092] INFO [Log partition=__consumer_offsets-19, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,092] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-49. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,092] INFO [Log partition=__consumer_offsets-49, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,092] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-31. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,092] INFO [Log partition=__consumer_offsets-31, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,093] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-28. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,093] INFO [Log partition=__consumer_offsets-28, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,093] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-25. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,093] INFO [Log partition=__consumer_offsets-25, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,093] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-7. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,093] INFO [Log partition=__consumer_offsets-7, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,093] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-40. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,093] INFO [Log partition=__consumer_offsets-40, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,093] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-37. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,093] INFO [Log partition=__consumer_offsets-37, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,093] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-4. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,093] INFO [Log partition=__consumer_offsets-4, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,093] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-1. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,093] INFO [Log partition=__consumer_offsets-1, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,093] WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-34. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,093] INFO [Log partition=__consumer_offsets-34, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,094] INFO [Log partition=__consumer_offsets-47, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,094] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-14. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,094] INFO [Log partition=__consumer_offsets-14, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,094] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-11. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,094] INFO [Log partition=__consumer_offsets-11, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,094] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-44. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,094] INFO [Log partition=__consumer_offsets-44, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,094] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-41. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,094] INFO [Log partition=__consumer_offsets-41, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,094] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-23. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,094] INFO [Log partition=__consumer_offsets-23, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,094] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-20. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,094] INFO [Log partition=__consumer_offsets-20, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,094] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-17. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-17, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,095] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-32. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-32, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,095] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-29. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-29, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,095] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-26. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-26, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,095] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-8. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-8, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,095] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-5. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-5, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,095] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-38. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-38, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,095] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-35. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-35, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,095] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Based on follower's leader epoch, leader replied with an unknown offset in __consumer_offsets-2. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2020-05-13 17:33:59,095] INFO [Log partition=__consumer_offsets-2, dir=/opt/kafka/data/logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2020-05-13 17:33:59,096] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,104] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,106] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,106] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,106] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,106] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,107] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,108] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,110] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partitions=[{topic=__consumer_offsets,partition=13,error_code=0},{topic=__consumer_offsets,partition=46,error_code=0},{topic=__consumer_offsets,partition=9,error_code=0},{topic=__consumer_offsets,partition=42,error_code=0},{topic=__consumer_offsets,partition=21,error_code=0},{topic=__consumer_offsets,partition=17,error_code=0},{topic=__consumer_offsets,partition=30,error_code=0},{topic=__consumer_offsets,partition=26,error_code=0},{topic=__consumer_offsets,partition=5,error_code=0},{topic=__consumer_offsets,partition=38,error_code=0},{topic=__consumer_offsets,partition=1,error_code=0},{topic=__consumer_offsets,partition=34,error_code=0},{topic=__consumer_offsets,partition=16,error_code=0},{topic=__consumer_offsets,partition=45,error_code=0},{topic=__consumer_offsets,partition=12,error_code=0},{topic=__consumer_offsets,partition=41,error_code=0},{topic=__consumer_offsets,partition=24,error_code=0},{topic=__consumer_offsets,partition=20,error_code=0},{topic=__consumer_offsets,partition=49,error_code=0},{topic=__consumer_offsets,partition=0,error_code=0},{topic=__consumer_offsets,partition=29,error_code=0},{topic=__consumer_offsets,partition=25,error_code=0},{topic=__consumer_offsets,partition=8,error_code=0},{topic=__consumer_offsets,partition=37,error_code=0},{topic=__consumer_offsets,partition=4,error_code=0},{topic=__consumer_offsets,partition=33,error_code=0},{topic=__consumer_offsets,partition=15,error_code=0},{topic=__consumer_offsets,partition=48,error_code=0},{topic=__consumer_offsets,partition=11,error_code=0},{topic=__consumer_offsets,partition=44,error_code=0},{topic=__consumer_offsets,partition=23,error_code=0},{topic=__consumer_offsets,partition=19,error_code=0},{topic=__consumer_offsets,partition=32,error_code=0},{topic=__consumer_offsets,partition=28,error_code=0},{topic=__consumer_offsets,partition=7,error_code=0},{topic=__consumer_offsets,partition=40,error_code=0},{topic=__consumer_offsets,partition=3,error_code=0},{topic=__consumer_offsets,partition=36,error_code=0},{topic=__consumer_offsets,partition=47,error_code=0},{topic=__consumer_offsets,partition=14,error_code=0},{topic=__consumer_offsets,partition=43,error_code=0},{topic=__consumer_offsets,partition=10,error_code=0},{topic=__consumer_offsets,partition=22,error_code=0},{topic=__consumer_offsets,partition=18,error_code=0},{topic=__consumer_offsets,partition=31,error_code=0},{topic=__consumer_offsets,partition=27,error_code=0},{topic=__consumer_offsets,partition=39,error_code=0},{topic=__consumer_offsets,partition=6,error_code=0},{topic=__consumer_offsets,partition=35,error_code=0},{topic=__consumer_offsets,partition=2,error_code=0}]} for request LEADER_AND_ISR with correlation id 7 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:33:59,111] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,112] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,113] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,115] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,116] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,116] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,117] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,117] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,116] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,117] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,117] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,117] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,117] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,117] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,117] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,117] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,118] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,119] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0, 2], zkVersion=0, replicas=[1, 0, 2], offlineReplicas=[]) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,120] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], offlineReplicas=[]) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,121] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,121] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2, 1], zkVersion=0, replicas=[0, 2, 1], offlineReplicas=[]) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,121] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1, 2], zkVersion=0, replicas=[0, 1, 2], offlineReplicas=[]) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,121] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1, 0], zkVersion=0, replicas=[2, 1, 0], offlineReplicas=[]) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,121] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0, 1], zkVersion=0, replicas=[2, 0, 1], offlineReplicas=[]) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8 (state.change.logger)
[2020-05-13 17:33:59,122] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 8 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:33:59,125] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-22. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,125] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-25. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,125] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-28. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,125] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-31. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,125] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-34. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,125] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-37. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,125] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-40. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,125] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-43. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-46. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-49. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-41. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-44. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-47. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-1. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-4. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-7. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-10. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-13. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-16. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-19. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-2. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-5. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-8. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-11. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-14. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-17. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-20. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-23. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-26. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-29. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-32. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-35. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,126] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-38. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:33:59,138] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-8 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,138] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-35 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,138] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-41 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,138] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-23 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,138] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-47 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,138] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-38 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-17 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-11 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-2 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-14 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-20 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-44 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-5 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-26 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-29 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,139] ERROR [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error for partition __consumer_offsets-32 at offset 0 (kafka.server.ReplicaFetcherThread)
org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[2020-05-13 17:33:59,430] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partitions=[{topic=__consumer_offsets,partition=13,error_code=0},{topic=__consumer_offsets,partition=46,error_code=0},{topic=__consumer_offsets,partition=9,error_code=0},{topic=__consumer_offsets,partition=42,error_code=0},{topic=__consumer_offsets,partition=21,error_code=0},{topic=__consumer_offsets,partition=17,error_code=0},{topic=__consumer_offsets,partition=30,error_code=0},{topic=__consumer_offsets,partition=26,error_code=0},{topic=__consumer_offsets,partition=5,error_code=0},{topic=__consumer_offsets,partition=38,error_code=0},{topic=__consumer_offsets,partition=1,error_code=0},{topic=__consumer_offsets,partition=34,error_code=0},{topic=__consumer_offsets,partition=16,error_code=0},{topic=__consumer_offsets,partition=45,error_code=0},{topic=__consumer_offsets,partition=12,error_code=0},{topic=__consumer_offsets,partition=41,error_code=0},{topic=__consumer_offsets,partition=24,error_code=0},{topic=__consumer_offsets,partition=20,error_code=0},{topic=__consumer_offsets,partition=49,error_code=0},{topic=__consumer_offsets,partition=0,error_code=0},{topic=__consumer_offsets,partition=29,error_code=0},{topic=__consumer_offsets,partition=25,error_code=0},{topic=__consumer_offsets,partition=8,error_code=0},{topic=__consumer_offsets,partition=37,error_code=0},{topic=__consumer_offsets,partition=4,error_code=0},{topic=__consumer_offsets,partition=33,error_code=0},{topic=__consumer_offsets,partition=15,error_code=0},{topic=__consumer_offsets,partition=48,error_code=0},{topic=__consumer_offsets,partition=11,error_code=0},{topic=__consumer_offsets,partition=44,error_code=0},{topic=__consumer_offsets,partition=23,error_code=0},{topic=__consumer_offsets,partition=19,error_code=0},{topic=__consumer_offsets,partition=32,error_code=0},{topic=__consumer_offsets,partition=28,error_code=0},{topic=__consumer_offsets,partition=7,error_code=0},{topic=__consumer_offsets,partition=40,error_code=0},{topic=__consumer_offsets,partition=3,error_code=0},{topic=__consumer_offsets,partition=36,error_code=0},{topic=__consumer_offsets,partition=47,error_code=0},{topic=__consumer_offsets,partition=14,error_code=0},{topic=__consumer_offsets,partition=43,error_code=0},{topic=__consumer_offsets,partition=10,error_code=0},{topic=__consumer_offsets,partition=22,error_code=0},{topic=__consumer_offsets,partition=18,error_code=0},{topic=__consumer_offsets,partition=31,error_code=0},{topic=__consumer_offsets,partition=27,error_code=0},{topic=__consumer_offsets,partition=39,error_code=0},{topic=__consumer_offsets,partition=6,error_code=0},{topic=__consumer_offsets,partition=35,error_code=0},{topic=__consumer_offsets,partition=2,error_code=0}]} for request LEADER_AND_ISR with correlation id 1 sent to broker 100.96.4.12:9092 (id: 2 rack: null) (state.change.logger)
[2020-05-13 17:33:59,436] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 2 sent to broker 100.96.4.12:9092 (id: 2 rack: null) (state.change.logger)
[2020-05-13 17:36:55,016] TRACE [Controller id=0] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2020-05-13 17:36:55,018] DEBUG [Controller id=0] Preferred replicas by broker Map(2 -> Map(__consumer_offsets-8 -> Vector(2, 0, 1), __consumer_offsets-35 -> Vector(2, 1, 0), __consumer_offsets-41 -> Vector(2, 1, 0), __consumer_offsets-23 -> Vector(2, 1, 0), __consumer_offsets-47 -> Vector(2, 1, 0), __consumer_offsets-38 -> Vector(2, 0, 1), __consumer_offsets-17 -> Vector(2, 1, 0), __consumer_offsets-11 -> Vector(2, 1, 0), __consumer_offsets-2 -> Vector(2, 0, 1), __consumer_offsets-14 -> Vector(2, 0, 1), __consumer_offsets-20 -> Vector(2, 0, 1), __consumer_offsets-44 -> Vector(2, 0, 1), __consumer_offsets-5 -> Vector(2, 1, 0), __consumer_offsets-26 -> Vector(2, 0, 1), __consumer_offsets-29 -> Vector(2, 1, 0), __consumer_offsets-32 -> Vector(2, 0, 1)), 1 -> Map(__consumer_offsets-22 -> Vector(1, 0, 2), __consumer_offsets-4 -> Vector(1, 0, 2), __consumer_offsets-7 -> Vector(1, 2, 0), __consumer_offsets-46 -> Vector(1, 0, 2), __consumer_offsets-25 -> Vector(1, 2, 0), __consumer_offsets-49 -> Vector(1, 2, 0), __consumer_offsets-16 -> Vector(1, 0, 2), __consumer_offsets-28 -> Vector(1, 0, 2), __consumer_offsets-31 -> Vector(1, 2, 0), __consumer_offsets-37 -> Vector(1, 2, 0), __consumer_offsets-19 -> Vector(1, 2, 0), __consumer_offsets-13 -> Vector(1, 2, 0), __consumer_offsets-43 -> Vector(1, 2, 0), connect-offsets-0 -> Vector(1), connect-status-0 -> Vector(1), __consumer_offsets-1 -> Vector(1, 2, 0), __consumer_offsets-34 -> Vector(1, 0, 2), __consumer_offsets-10 -> Vector(1, 0, 2), __consumer_offsets-40 -> Vector(1, 0, 2)), 0 -> Map(__consumer_offsets-30 -> Vector(0, 1, 2), __consumer_offsets-21 -> Vector(0, 2, 1), __consumer_offsets-27 -> Vector(0, 2, 1), __consumer_offsets-9 -> Vector(0, 2, 1), __consumer_offsets-33 -> Vector(0, 2, 1), connect-configs-0 -> Vector(0), __consumer_offsets-36 -> Vector(0, 1, 2), __consumer_offsets-42 -> Vector(0, 1, 2), __consumer_offsets-3 -> Vector(0, 2, 1), __consumer_offsets-18 -> Vector(0, 1, 2), __consumer_offsets-15 -> Vector(0, 2, 1), __consumer_offsets-24 -> Vector(0, 1, 2), __consumer_offsets-48 -> Vector(0, 1, 2), __consumer_offsets-6 -> Vector(0, 1, 2), __consumer_offsets-0 -> Vector(0, 1, 2), __consumer_offsets-39 -> Vector(0, 2, 1), __consumer_offsets-12 -> Vector(0, 1, 2), __consumer_offsets-45 -> Vector(0, 2, 1))) (kafka.controller.KafkaController)
[2020-05-13 17:36:55,019] DEBUG [Controller id=0] Topics not in preferred replica for broker 2 Map() (kafka.controller.KafkaController)
[2020-05-13 17:36:55,020] TRACE [Controller id=0] Leader imbalance ratio for broker 2 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:36:55,020] DEBUG [Controller id=0] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2020-05-13 17:36:55,020] TRACE [Controller id=0] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:36:55,020] DEBUG [Controller id=0] Topics not in preferred replica for broker 0 Map() (kafka.controller.KafkaController)
[2020-05-13 17:36:55,020] TRACE [Controller id=0] Leader imbalance ratio for broker 0 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:40:28,127] INFO [Controller id=0] New topics: [Set(postgres.public.model_versions)], deleted topics: [Set()], new partition replica assignment [Map(postgres.public.model_versions-0 -> Vector(1))] (kafka.controller.KafkaController)
[2020-05-13 17:40:28,128] INFO [Controller id=0] New partition creation callback for postgres.public.model_versions-0 (kafka.controller.KafkaController)
[2020-05-13 17:40:28,128] TRACE [Controller id=0 epoch=1] Changed partition postgres.public.model_versions-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2020-05-13 17:40:28,128] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition postgres.public.model_versions-0 from NonExistentReplica to NewReplica (state.change.logger)
[2020-05-13 17:40:28,138] TRACE [Controller id=0 epoch=1] Changed partition postgres.public.model_versions-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[2020-05-13 17:40:28,138] TRACE [Controller id=0 epoch=1] Sending become-leader LeaderAndIsr request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=1, zkVersion=0, replicas=1, isNew=true) to broker 1 for partition postgres.public.model_versions-0 (state.change.logger)
[2020-05-13 17:40:28,138] TRACE [Controller id=0 epoch=1] Sending UpdateMetadata request PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) to brokers Set(0, 1, 2) for partition postgres.public.model_versions-0 (state.change.logger)
[2020-05-13 17:40:28,139] TRACE [Controller id=0 epoch=1] Changed state of replica 1 for partition postgres.public.model_versions-0 from NewReplica to OnlineReplica (state.change.logger)
[2020-05-13 17:40:28,139] TRACE [Broker id=0] Cached leader info PartitionState(controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition postgres.public.model_versions-0 in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 9 (state.change.logger)
[2020-05-13 17:40:28,141] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 3 sent to broker 100.96.4.12:9092 (id: 2 rack: null) (state.change.logger)
[2020-05-13 17:40:28,142] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 9 sent to broker 100.96.2.11:9092 (id: 0 rack: null) (state.change.logger)
[2020-05-13 17:40:28,154] TRACE [Controller id=0 epoch=1] Received response {error_code=0,partitions=[{topic=postgres.public.model_versions,partition=0,error_code=0}]} for request LEADER_AND_ISR with correlation id 9 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:40:28,156] TRACE [Controller id=0 epoch=1] Received response {error_code=0} for request UPDATE_METADATA with correlation id 10 sent to broker 100.96.5.7:9092 (id: 1 rack: null) (state.change.logger)
[2020-05-13 17:41:49,757] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:41:55,020] TRACE [Controller id=0] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2020-05-13 17:41:55,021] DEBUG [Controller id=0] Preferred replicas by broker Map(2 -> Map(__consumer_offsets-8 -> Vector(2, 0, 1), __consumer_offsets-35 -> Vector(2, 1, 0), __consumer_offsets-41 -> Vector(2, 1, 0), __consumer_offsets-23 -> Vector(2, 1, 0), __consumer_offsets-47 -> Vector(2, 1, 0), __consumer_offsets-38 -> Vector(2, 0, 1), __consumer_offsets-17 -> Vector(2, 1, 0), __consumer_offsets-11 -> Vector(2, 1, 0), __consumer_offsets-2 -> Vector(2, 0, 1), __consumer_offsets-14 -> Vector(2, 0, 1), __consumer_offsets-20 -> Vector(2, 0, 1), __consumer_offsets-44 -> Vector(2, 0, 1), __consumer_offsets-5 -> Vector(2, 1, 0), __consumer_offsets-26 -> Vector(2, 0, 1), __consumer_offsets-29 -> Vector(2, 1, 0), __consumer_offsets-32 -> Vector(2, 0, 1)), 1 -> Map(__consumer_offsets-22 -> Vector(1, 0, 2), __consumer_offsets-4 -> Vector(1, 0, 2), __consumer_offsets-7 -> Vector(1, 2, 0), __consumer_offsets-46 -> Vector(1, 0, 2), __consumer_offsets-25 -> Vector(1, 2, 0), __consumer_offsets-49 -> Vector(1, 2, 0), __consumer_offsets-16 -> Vector(1, 0, 2), __consumer_offsets-28 -> Vector(1, 0, 2), __consumer_offsets-31 -> Vector(1, 2, 0), __consumer_offsets-37 -> Vector(1, 2, 0), __consumer_offsets-19 -> Vector(1, 2, 0), __consumer_offsets-13 -> Vector(1, 2, 0), __consumer_offsets-43 -> Vector(1, 2, 0), connect-offsets-0 -> Vector(1), connect-status-0 -> Vector(1), postgres.public.model_versions-0 -> Vector(1), __consumer_offsets-1 -> Vector(1, 2, 0), __consumer_offsets-34 -> Vector(1, 0, 2), __consumer_offsets-10 -> Vector(1, 0, 2), __consumer_offsets-40 -> Vector(1, 0, 2)), 0 -> Map(__consumer_offsets-30 -> Vector(0, 1, 2), __consumer_offsets-21 -> Vector(0, 2, 1), __consumer_offsets-27 -> Vector(0, 2, 1), __consumer_offsets-9 -> Vector(0, 2, 1), __consumer_offsets-33 -> Vector(0, 2, 1), connect-configs-0 -> Vector(0), __consumer_offsets-36 -> Vector(0, 1, 2), __consumer_offsets-42 -> Vector(0, 1, 2), __consumer_offsets-3 -> Vector(0, 2, 1), __consumer_offsets-18 -> Vector(0, 1, 2), __consumer_offsets-15 -> Vector(0, 2, 1), __consumer_offsets-24 -> Vector(0, 1, 2), __consumer_offsets-48 -> Vector(0, 1, 2), __consumer_offsets-6 -> Vector(0, 1, 2), __consumer_offsets-0 -> Vector(0, 1, 2), __consumer_offsets-39 -> Vector(0, 2, 1), __consumer_offsets-12 -> Vector(0, 1, 2), __consumer_offsets-45 -> Vector(0, 2, 1))) (kafka.controller.KafkaController)
[2020-05-13 17:41:55,022] DEBUG [Controller id=0] Topics not in preferred replica for broker 2 Map() (kafka.controller.KafkaController)
[2020-05-13 17:41:55,022] TRACE [Controller id=0] Leader imbalance ratio for broker 2 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:41:55,022] DEBUG [Controller id=0] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2020-05-13 17:41:55,023] TRACE [Controller id=0] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:41:55,023] DEBUG [Controller id=0] Topics not in preferred replica for broker 0 Map() (kafka.controller.KafkaController)
[2020-05-13 17:41:55,023] TRACE [Controller id=0] Leader imbalance ratio for broker 0 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:46:55,023] TRACE [Controller id=0] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2020-05-13 17:46:55,024] DEBUG [Controller id=0] Preferred replicas by broker Map(2 -> Map(__consumer_offsets-8 -> Vector(2, 0, 1), __consumer_offsets-35 -> Vector(2, 1, 0), __consumer_offsets-41 -> Vector(2, 1, 0), __consumer_offsets-23 -> Vector(2, 1, 0), __consumer_offsets-47 -> Vector(2, 1, 0), __consumer_offsets-38 -> Vector(2, 0, 1), __consumer_offsets-17 -> Vector(2, 1, 0), __consumer_offsets-11 -> Vector(2, 1, 0), __consumer_offsets-2 -> Vector(2, 0, 1), __consumer_offsets-14 -> Vector(2, 0, 1), __consumer_offsets-20 -> Vector(2, 0, 1), __consumer_offsets-44 -> Vector(2, 0, 1), __consumer_offsets-5 -> Vector(2, 1, 0), __consumer_offsets-26 -> Vector(2, 0, 1), __consumer_offsets-29 -> Vector(2, 1, 0), __consumer_offsets-32 -> Vector(2, 0, 1)), 1 -> Map(__consumer_offsets-22 -> Vector(1, 0, 2), __consumer_offsets-4 -> Vector(1, 0, 2), __consumer_offsets-7 -> Vector(1, 2, 0), __consumer_offsets-46 -> Vector(1, 0, 2), __consumer_offsets-25 -> Vector(1, 2, 0), __consumer_offsets-49 -> Vector(1, 2, 0), __consumer_offsets-16 -> Vector(1, 0, 2), __consumer_offsets-28 -> Vector(1, 0, 2), __consumer_offsets-31 -> Vector(1, 2, 0), __consumer_offsets-37 -> Vector(1, 2, 0), __consumer_offsets-19 -> Vector(1, 2, 0), __consumer_offsets-13 -> Vector(1, 2, 0), __consumer_offsets-43 -> Vector(1, 2, 0), connect-offsets-0 -> Vector(1), connect-status-0 -> Vector(1), postgres.public.model_versions-0 -> Vector(1), __consumer_offsets-1 -> Vector(1, 2, 0), __consumer_offsets-34 -> Vector(1, 0, 2), __consumer_offsets-10 -> Vector(1, 0, 2), __consumer_offsets-40 -> Vector(1, 0, 2)), 0 -> Map(__consumer_offsets-30 -> Vector(0, 1, 2), __consumer_offsets-21 -> Vector(0, 2, 1), __consumer_offsets-27 -> Vector(0, 2, 1), __consumer_offsets-9 -> Vector(0, 2, 1), __consumer_offsets-33 -> Vector(0, 2, 1), connect-configs-0 -> Vector(0), __consumer_offsets-36 -> Vector(0, 1, 2), __consumer_offsets-42 -> Vector(0, 1, 2), __consumer_offsets-3 -> Vector(0, 2, 1), __consumer_offsets-18 -> Vector(0, 1, 2), __consumer_offsets-15 -> Vector(0, 2, 1), __consumer_offsets-24 -> Vector(0, 1, 2), __consumer_offsets-48 -> Vector(0, 1, 2), __consumer_offsets-6 -> Vector(0, 1, 2), __consumer_offsets-0 -> Vector(0, 1, 2), __consumer_offsets-39 -> Vector(0, 2, 1), __consumer_offsets-12 -> Vector(0, 1, 2), __consumer_offsets-45 -> Vector(0, 2, 1))) (kafka.controller.KafkaController)
[2020-05-13 17:46:55,024] DEBUG [Controller id=0] Topics not in preferred replica for broker 2 Map() (kafka.controller.KafkaController)
[2020-05-13 17:46:55,024] TRACE [Controller id=0] Leader imbalance ratio for broker 2 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:46:55,025] DEBUG [Controller id=0] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2020-05-13 17:46:55,025] TRACE [Controller id=0] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:46:55,025] DEBUG [Controller id=0] Topics not in preferred replica for broker 0 Map() (kafka.controller.KafkaController)
[2020-05-13 17:46:55,025] TRACE [Controller id=0] Leader imbalance ratio for broker 0 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:51:49,757] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-05-13 17:51:55,025] TRACE [Controller id=0] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[2020-05-13 17:51:55,028] DEBUG [Controller id=0] Preferred replicas by broker Map(2 -> Map(__consumer_offsets-8 -> Vector(2, 0, 1), __consumer_offsets-35 -> Vector(2, 1, 0), __consumer_offsets-41 -> Vector(2, 1, 0), __consumer_offsets-23 -> Vector(2, 1, 0), __consumer_offsets-47 -> Vector(2, 1, 0), __consumer_offsets-38 -> Vector(2, 0, 1), __consumer_offsets-17 -> Vector(2, 1, 0), __consumer_offsets-11 -> Vector(2, 1, 0), __consumer_offsets-2 -> Vector(2, 0, 1), __consumer_offsets-14 -> Vector(2, 0, 1), __consumer_offsets-20 -> Vector(2, 0, 1), __consumer_offsets-44 -> Vector(2, 0, 1), __consumer_offsets-5 -> Vector(2, 1, 0), __consumer_offsets-26 -> Vector(2, 0, 1), __consumer_offsets-29 -> Vector(2, 1, 0), __consumer_offsets-32 -> Vector(2, 0, 1)), 1 -> Map(__consumer_offsets-22 -> Vector(1, 0, 2), __consumer_offsets-4 -> Vector(1, 0, 2), __consumer_offsets-7 -> Vector(1, 2, 0), __consumer_offsets-46 -> Vector(1, 0, 2), __consumer_offsets-25 -> Vector(1, 2, 0), __consumer_offsets-49 -> Vector(1, 2, 0), __consumer_offsets-16 -> Vector(1, 0, 2), __consumer_offsets-28 -> Vector(1, 0, 2), __consumer_offsets-31 -> Vector(1, 2, 0), __consumer_offsets-37 -> Vector(1, 2, 0), __consumer_offsets-19 -> Vector(1, 2, 0), __consumer_offsets-13 -> Vector(1, 2, 0), __consumer_offsets-43 -> Vector(1, 2, 0), connect-offsets-0 -> Vector(1), connect-status-0 -> Vector(1), postgres.public.model_versions-0 -> Vector(1), __consumer_offsets-1 -> Vector(1, 2, 0), __consumer_offsets-34 -> Vector(1, 0, 2), __consumer_offsets-10 -> Vector(1, 0, 2), __consumer_offsets-40 -> Vector(1, 0, 2)), 0 -> Map(__consumer_offsets-30 -> Vector(0, 1, 2), __consumer_offsets-21 -> Vector(0, 2, 1), __consumer_offsets-27 -> Vector(0, 2, 1), __consumer_offsets-9 -> Vector(0, 2, 1), __consumer_offsets-33 -> Vector(0, 2, 1), connect-configs-0 -> Vector(0), __consumer_offsets-36 -> Vector(0, 1, 2), __consumer_offsets-42 -> Vector(0, 1, 2), __consumer_offsets-3 -> Vector(0, 2, 1), __consumer_offsets-18 -> Vector(0, 1, 2), __consumer_offsets-15 -> Vector(0, 2, 1), __consumer_offsets-24 -> Vector(0, 1, 2), __consumer_offsets-48 -> Vector(0, 1, 2), __consumer_offsets-6 -> Vector(0, 1, 2), __consumer_offsets-0 -> Vector(0, 1, 2), __consumer_offsets-39 -> Vector(0, 2, 1), __consumer_offsets-12 -> Vector(0, 1, 2), __consumer_offsets-45 -> Vector(0, 2, 1))) (kafka.controller.KafkaController)
[2020-05-13 17:51:55,028] DEBUG [Controller id=0] Topics not in preferred replica for broker 2 Map() (kafka.controller.KafkaController)
[2020-05-13 17:51:55,028] TRACE [Controller id=0] Leader imbalance ratio for broker 2 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:51:55,028] DEBUG [Controller id=0] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
[2020-05-13 17:51:55,028] TRACE [Controller id=0] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[2020-05-13 17:51:55,028] DEBUG [Controller id=0] Topics not in preferred replica for broker 0 Map() (kafka.controller.KafkaController)
[2020-05-13 17:51:55,028] TRACE [Controller id=0] Leader imbalance ratio for broker 0 is 0.0 (kafka.controller.KafkaController)
